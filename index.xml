<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bheisler.github.io</title>
    <link>https://bheisler.github.io/index.xml</link>
    <description>Recent content on bheisler.github.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Apr 2017 00:00:00 -0600</lastBuildDate>
    <atom:link href="https://bheisler.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Calling Rust From Python</title>
      <link>https://bheisler.github.io/post/calling-rust-in-python/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/calling-rust-in-python/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Hello! This is a detailed example of exposing Rust code to other languages (in
this case, Python). Most articles I&amp;rsquo;ve seen that cover this topic uses really
trivial example functions, skipping over a lot of the complexity. Even the better
ones out there typically don&amp;rsquo;t have a pre-existing, reasonably complex program
to work with. I&amp;rsquo;m going to start with trivial functions and build my way up to
being able to define a scene for my &lt;a href=&#34;https://github.com/bheisler/raytracer&#34;&gt;raytracer&lt;/a&gt;
in Python using a series of calls to Rust, then render it and return the
resulting image data back to Python. If you want to know more about the raytracer,
I wrote a series of posts on it &lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/&#34;&gt;here&lt;/a&gt;,
but it won&amp;rsquo;t be necessary; I&amp;rsquo;ll explain parts of the raytracer here as we need
them. Hopefully this will give a more complete picture of how to incorporate
complex Rust code into Python.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve never written any sort of Python/C interop before, so this should be another
learning experience all around. I&amp;rsquo;m going to arbitrarily choose
&lt;a href=&#34;https://cffi.readthedocs.io/en/latest/&#34;&gt;CFFI&lt;/a&gt; as the Python interop library.
It&amp;rsquo;s portable across interpreters and seems nicer to use than &lt;a href=&#34;https://docs.python.org/2/library/ctypes.html&#34;&gt;ctypes&lt;/a&gt;.
I expect the main concepts will be broadly applicable to other libraries (and
other languages such as Ruby). Let get started!&lt;/p&gt;

&lt;h2 id=&#34;calling-functions&#34;&gt;Calling Functions&lt;/h2&gt;

&lt;p&gt;The first thing to do is to define a Rust function we want to call from Python.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/effc8c457c9d85d1e318be52e1b8c98d.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;We&amp;rsquo;re actually defining a function for Rust&amp;rsquo;s C foreign-function interface. The
basic idea here is that we write a wrapper in Python that knows how to call
C functions, and a wrapper in Rust that exposes C functions and translates them
to regular function calls in Rust. It&amp;rsquo;s sort of like we&amp;rsquo;re calling from Python
into C into Rust. The &lt;code&gt;no_mangle&lt;/code&gt; attribute and &lt;code&gt;extern &amp;quot;C&amp;quot;&lt;/code&gt; above instruct rustc
not to change the name of the function (otherwise CFFI wouldn&amp;rsquo;t be able to
find it later) and to emit a function that can be called as if it were written
in C. We&amp;rsquo;ll need both for all functions that we want to expose to C.&lt;/p&gt;

&lt;p&gt;Now we need to instruct Cargo to build this library as a dynamic library
(&amp;ldquo;dylib&amp;rdquo; in Cargo terms). I&amp;rsquo;m writing this on a Windows PC, so Cargo
produces a &lt;code&gt;raytracer_ffi.dll&lt;/code&gt; file. I tested it on Linux as well and it created
&lt;code&gt;libraytracer_ffi.so&lt;/code&gt;.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/06c25b67a35bfd8f5b38781256558230.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then we need some Python code to load and call this shared library:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/ec798db12cd69153a6330e67eb6d3dac.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Let&amp;rsquo;s break this down a bit. First we import the &lt;code&gt;cffi&lt;/code&gt; module and create an
FFI object. Then we call &lt;code&gt;cdef&lt;/code&gt; and pass it some text - this text is a C function
signature matching the &lt;code&gt;double&lt;/code&gt; function in Rust. CFFI parses this function
signature in order to determine how to call the function. We&amp;rsquo;ll need to do this
for all of the functions and structs we want to expose to Python. Then we open
the DLL file with &lt;code&gt;dlopen&lt;/code&gt;. Finally, we call the &lt;code&gt;double&lt;/code&gt; function as if it were
a regular Python function and print the result.&lt;/p&gt;

&lt;p&gt;And now we should be able to call &lt;code&gt;double&lt;/code&gt; from Python:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe test.py
18
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Side note: I wasn&amp;rsquo;t able to get this working with PyPy on 64-bit Windows. I
didn&amp;rsquo;t find out why, but I assume it has something to do with how PyPy only
provides 32-bit binaries. PyPy worked fine for me on Linux, but I had to use
64-bit CPython on Windows.&lt;/p&gt;

&lt;h2 id=&#34;passing-structures&#34;&gt;Passing Structures&lt;/h2&gt;

&lt;p&gt;Now, if I&amp;rsquo;m going to be able to define a scene in Python, I&amp;rsquo;ll need to be able
to call functions and pass in structs as arguments. I&amp;rsquo;ll keep working with this
toy program a bit longer, but instead of simply doubling an integer, let&amp;rsquo;s try
and get it to calculate the length of a vector using &lt;code&gt;vector::Vector3::length&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;First, I&amp;rsquo;ll need to tell rustc that Vector3 should be laid out like a C struct.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/23de3e8f86143ceea2240b2a283b8f91.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;It appears that CFFI doesn&amp;rsquo;t have any way to call functions with stack-allocated
structures. Using the stack for small, copyable structures like Vector3 is
pretty common in Rust, but I guess it isn&amp;rsquo;t in C? So instead, our Rust function
will have to accept a pointer to a Vector3.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/f02545b55a01f5602f9aa8802c970847.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Here we define an extern function which accepts a raw pointer to a Vector3.
Dereferencing raw pointers is unsafe, so we use an unsafe block to convert the
raw pointer to a Rust reference. Finally, we call &lt;code&gt;length()&lt;/code&gt; and return the value.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/d7c3c411f6826303a8a09868821b6829.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Back in Python-land, we define a structure type matching Vector3 and the
signature of the length function. Now we need to allocate a new vector_t object,
which is done with the &lt;code&gt;ffi.new()&lt;/code&gt; function. We need to pay attention to
ownership here - the memory for the vector_t is allocated by Python and it will
have to be freed by Python. In this case, it will be freed when the vector object
gets garbage collected so we don&amp;rsquo;t need to worry about it, but we&amp;rsquo;ll need to
be more careful about ownership later.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe test.py
1.73205080757
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;returning-references-back-to-python&#34;&gt;Returning References Back To Python&lt;/h2&gt;

&lt;p&gt;Now we&amp;rsquo;ll start the process of building our actual FFI code. We&amp;rsquo;ll start with the
Scene structure. I don&amp;rsquo;t especially want to expose all the complexity of the
Scene structure to Python, so instead we&amp;rsquo;ll use another C idiom and return an
opaque pointer.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/9a552f490d320410b28ab5e6c065ee9f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Notice that we use &lt;code&gt;Box::new&lt;/code&gt; to heap-allocate the structure, and &lt;code&gt;Box::into_raw&lt;/code&gt;
to convert it into a raw pointer to return. The corresponding Python code is:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/fcdb3bc11c85147172e1a6ec42f224d0.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;I&amp;rsquo;m not actually sure &lt;code&gt;void*&lt;/code&gt; is the right way to go here, but I don&amp;rsquo;t know any
other way to do opaque pointers in this situation. If you know more about this,
let me know. CFFI seems to understand &lt;code&gt;uint32_t&lt;/code&gt; all on its own, and presumably
will call the Rust function with the appropriate integer width.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe raytracer.py
From Rust: Scene { width: 800, height: 600, fov: 45, elements: [],
    lights: [], shadow_bias: 0.0000000000001, max_recursion_depth: 10 }
From Python: &amp;lt;cdata &#39;void*&#39; 0x000000000155B260&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sharp readers might have noticed that we&amp;rsquo;re leaking Scene objects - we&amp;rsquo;re
allocating some memory on the heap for the boxed Scene and never freeing it.
For this trivial example, it doesn&amp;rsquo;t matter much because it will be cleaned
up when the process terminates, but it is rather inelegant, so let&amp;rsquo;s fix that.&lt;/p&gt;

&lt;h2 id=&#34;disposing-of-allocated-objects&#34;&gt;Disposing Of Allocated Objects&lt;/h2&gt;

&lt;p&gt;This goes back to the brief discussion of ownership earlier. Previously,
Python owned the allocated Vector3 object, so we could trust that it would be
safely freed when it was garbage-collected. Now, we have an object allocated by
Rust, but owned by a pointer in Python. Python doesn&amp;rsquo;t know how to deallocate
an object owned by Rust, so we&amp;rsquo;ll have to return ownership of the pointer to
Rust and allow Rust to free the memory.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2f98fb5590e4dcd895773b1dd39a100d.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Freeing the memory is actually quite simple - we use &lt;code&gt;Box::from_raw&lt;/code&gt; to convert
the raw pointer back into a box, and then just let it fall out of scope. Rust
will automatically clean everything up for us.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/51656ea2be9b8d0c2e7ba2c6ba77bfe4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Right now, there&amp;rsquo;s nothing to stop us from freeing the scene more than once,
or continuing to use that pointer after the scene has been freed. There&amp;rsquo;s nothing
we can do about that from the Rust side, but in Python we can at least build a
safe wrapper to work with.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/ba18f7b5c73c66ad9d1924e80881fc9e.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Here, we define a Python class to represent our Scene. It defines the &lt;code&gt;__enter__&lt;/code&gt;
and &lt;code&gt;__exit__&lt;/code&gt; methods necessary to act as a &lt;a href=&#34;https://www.python.org/dev/peps/pep-0343/&#34;&gt;Context Manager&lt;/a&gt;,
which allows us to use it with the &lt;code&gt;with&lt;/code&gt; statement at the end. Running this
file confirms that the scene object is being freed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe raytracer.py
Freeing the scene
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;enums&#34;&gt;Enums&lt;/h2&gt;

&lt;p&gt;Before we begin constructing our scene in Python, however, there&amp;rsquo;s one more bit
of complexity to tackle first. Every object in this raytracer contains a Material
structure to define what color the surface is, whether it&amp;rsquo;s reflective or
transparent, etc. This is defined in Rust using some enums and a struct:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/5af67a2bc9fb63ad1c77e087d5857c91.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Rust&amp;rsquo;s enums have no equivalent in C, and even if they did that DynamicImage
type certainly doesn&amp;rsquo;t. We&amp;rsquo;ll have to create C-compatible wrappers for these
types that we can expose to Python. I&amp;rsquo;ll focus on the Coloration enum for now,
the SurfaceType enum will work the same way.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start by defining another enum:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/153001fea515624f8eaaf807640c79bb.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;I know, I just said we can&amp;rsquo;t do enums in C. Instead, we&amp;rsquo;ll define a couple of
functions to create CColoration values on the heap and return opaque pointers
to them like we did with the Scene.&lt;/p&gt;

&lt;p&gt;First, the simple case of a solid color:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/37004afc200aa009204031f1443a2ad9.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then, the more complex case of a path to a texture file.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/8fc77676d46de395b661c89bb0f5384d.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Here we take a pointer to a null-terminated character array (a C-style string)
and convert it to a Rust string, which has a length and is encoded in UTF-8.
This conversion could fail, if the C string isn&amp;rsquo;t valid UTF-8. Notice that we
need to be very careful not to panic. We can&amp;rsquo;t just unwrap the result of
converting the CStr to a regular string, because panicking across FFI boundaries
is undefined behavior. Instead, we return a null pointer on all error
conditions. A more serious project would probably want to have more robust error
handling, but this is sufficient for now.&lt;/p&gt;

&lt;p&gt;The corresponding Python should be relatively familiar by now:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/02f53a18cc343e57b3a6e9b0c31678a4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The SurfaceType enum works basically the same way as above, so I&amp;rsquo;ll spare you
the details.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/fbfb35e8b010c89e7b40cc5b93b65a2c.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;All those with&amp;rsquo;s are kind of ugly, but that&amp;rsquo;s the price we pay for safety.
We can verify that everything is being freed as expected:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe raytracer.py
Freeing surface type
Freeing surface type
Freeing surface type
Freeing coloration
Freeing coloration
Freeing coloration
Freeing coloration
Freeing coloration
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;constructing-the-scene&#34;&gt;Constructing the Scene&lt;/h2&gt;

&lt;p&gt;Finally, we&amp;rsquo;re ready to start constructing the scene. I&amp;rsquo;ll focus on the case of
adding a Sphere to the scene. The code to define other objects is pretty much
the same.&lt;/p&gt;

&lt;p&gt;First, we need a new struct to represent Material:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/b90e0a5de2bdbc16d0120142bf6b94ba.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;And a function to add a sphere to a scene:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/bede2aa436be20ede3c7c876f2f28488.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Most of this is the now-familiar C foreign-function boilerplate. The
&lt;code&gt;material.to_rust()&lt;/code&gt; method works pretty much as you&amp;rsquo;d expect - it
constructs a Material value from a CMaterial value, potentially loading the
texture contained in the &lt;code&gt;CColoration&lt;/code&gt;. More noteworthy is the way we convert
the scene Box back into a raw pointer at the end of the method. This prevents
Rust from deallocating our scene.&lt;/p&gt;

&lt;p&gt;You might reasonably ask why I chose to have one function that creates and adds
the sphere directly to the scene. This does, after all, make it impossible for
me to return a Sphere to Python. The answer is that since I don&amp;rsquo;t really want to
manipulate Spheres in Python, there&amp;rsquo;s not much point in going to all that extra
effort. You can go ahead and do that if you like.&lt;/p&gt;

&lt;p&gt;Now that we have all of that, we can call it from Python as before:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/690a6cc77c1e3ba295f5ce45798b6f26.js&#34;&gt;&lt;/script&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe raytracer.py
Sphere { center: Point { x: 0, y: 0, z: -5 }, radius: 1, material:
  Material { coloration: Texture, albedo: 0.18, surface:
  Reflective { reflectivity: 0.7 } } }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;rendering-and-returning-the-image-to-python&#34;&gt;Rendering and Returning the Image To Python&lt;/h2&gt;

&lt;p&gt;Now that we can define a scene in Python, we need a way to render it and return
the resulting image. We can&amp;rsquo;t just return a byte array, because Python can&amp;rsquo;t
handle stack-allocated objects, and anyway it would overflow the stack. We could
return a pointer/length pair, but then we have to pass it back to Rust to free
it. Instead, we&amp;rsquo;ll follow the C convention and have the caller provide a buffer
to render the image into.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/5c3983559358ef40d4de64d83a44cf61.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;After the usual boilerplate, we convert the C-style byte array into a mutable
slice with the &lt;code&gt;slice::from_raw_parts_mut&lt;/code&gt; function, then wrap that into an
ImageBuffer and pass it to the raytracer for rendering. Slices in Rust don&amp;rsquo;t
own their contents, so we don&amp;rsquo;t need to do anything special to prevent Rust from
trying to free the buffer.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/51eb8b1f5ad6067942d8a1a969842185.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;In Python, we need to save the dimensions of the image so that we can allocate
an appropriate buffer. The raytracer uses 4-byte RGBA pixels, so we calculate
the buffer size as 4 * width * height, allocate an appropriate buffer, and
render the image into it. Then we call &lt;code&gt;ffi.buffer&lt;/code&gt; to wrap it into a convenient
Python object. Finally, we pass that to the Pillow library to be wrapped into
an Image object that we can save out to disk or do further processing on.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://imgur.com/a/knyif&#34;&gt;&lt;img src=&#34;https://bheisler.github.io/static/rendered-by-python.png&#34; alt=&#34;Rendered By Python&#34; /&gt;&lt;/a&gt;
Click to see high-resolution image&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Overall, this turned out to be easier than I&amp;rsquo;d expected. CFFI&amp;rsquo;s user-friendly
interface helped a lot, I think, though the Rust side has a lot of boilerplate.
I expect some macros or something could help with that. I&amp;rsquo;d like to thank Jake
Goulding and co. for the &lt;a href=&#34;http://jakegoulding.com/rust-ffi-omnibus/slice_arguments/&#34;&gt;Rust FFI Omnibus&lt;/a&gt;,
which covers all of the basic techniques listed above (and provides examples
for a number of other languages, if you&amp;rsquo;d like to compare).&lt;/p&gt;

&lt;p&gt;As usual, if you want to try playing around with the code yourself, you can
check out the &lt;a href=&#34;https://github.com/bheisler/raytracer&#34;&gt;GitHub Repository&lt;/a&gt;. If you
do, though, be careful with the complexity of the scene you try to render. It&amp;rsquo;s
very easy to reach multi-hour rendering times when you&amp;rsquo;re defining scenes
programmatically. Otherwise, enjoy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing a Raytracer in Rust - Part 3 - Reflection and Refraction</title>
      <link>https://bheisler.github.io/post/writing-raytracer-in-rust-part-3/</link>
      <pubDate>Mon, 27 Mar 2017 00:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/writing-raytracer-in-rust-part-3/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Hello again, and welcome to the final part of my series on writing a raytracer
in Rust (&lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/&#34;&gt;Part 1&lt;/a&gt;,
&lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-2/&#34;&gt;Part 2&lt;/a&gt;). Previously we implemented
a basic raytracer which could handle diffuse shading of planes and spheres with
multiple objects and multiple lights. This time, we&amp;rsquo;ll add texturing, reflection
and transparent objects.&lt;/p&gt;

&lt;p&gt;First, I&amp;rsquo;ve refactored the common parts of Sphere and Plane out to a separate
structure. Since this post is all about handling more complex surface properties,
we&amp;rsquo;ll need a structure to represent them and avoid duplication.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/f49634600ec7910c149e577804a5e3cc.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;texturing&#34;&gt;Texturing&lt;/h2&gt;

&lt;p&gt;In order to texture our objects, we need to do two things. First, we need to
calculate the texture coordinates corresponding to the point on the object that
the ray intersected. Then we need to look up the color at those coordinates.
We&amp;rsquo;ll start by introducing a structure to contain our texture coordinates and a
function to calculate them.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/952f43be37463e22d7db93800f12ee21.js&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;spheres&#34;&gt;Spheres&lt;/h3&gt;

&lt;p&gt;The texture coordinates of a sphere are simply the spherical coordinates of the
intersection point. If our sphere were the Earth, these would be akin to the
latitude and longitude.&lt;/p&gt;

&lt;p&gt;We can compute the (x, y, z) coordinates of the intersection relative to the
center of the sphere by using the vector subtraction of the hit point and the
center of the sphere. Then, we can convert those to the spherical coordinates
using these formulas:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;phi = atan(z, x)
theta = acos(y/R) //Where R is the radius of the sphere
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If your trigonometry is rusty, check out &lt;a href=&#34;https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/spherical-coordinates-and-trigonometric-functions&#34;&gt;this explanation&lt;/a&gt;
at (you guessed it)
Scratchapixel. If you do though, be aware that their coordinates have the Z and
Y axes swapped so they have slightly different formulas.&lt;/p&gt;

&lt;p&gt;These formulas produce values in the range (-pi&amp;hellip;pi) and (0&amp;hellip;pi) respectively.
We want (0&amp;hellip;1) instead so we&amp;rsquo;ll adjust the formula to remap the values to the
correct range:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tex.x = (1 + atan(z, x) / pi) * 0.5
tex.y = acos(y) / pi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have something we can implement in code, like so:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/fe8a5ba0f4ee0f7d835b100126220636.js&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;planes&#34;&gt;Planes&lt;/h3&gt;

&lt;p&gt;To calculate the texture coordinates on a plane, we first need to construct a
coordinate system (that is, two perpendicular unit vectors) aligned with that
plane. This can be done using the cross product, which takes two vectors and
produces a new vector which is perpendicular to them. In this case, we use the
surface normal and the forward vector (unless the normal is equal to the forward
vector, in which case use the up vector). This produces one vector parallel to
the plane to be our X axis. To get the other vector, we can just cross the
normal with the X axis.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/0c2213af556812edd3bae4f9827a666c.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then we can compute the texture coordinates by taking the dot product of the
vector from the hit location to the origin against the axes (effectively
separating the hit vector into its X and Y components).&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/c6f60e809599459aa216bfb0391b3adf.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;It might be useful to add a scaling factor and an offset to allow the user to
adjust the position and size of the texture, but this is left as an exercise for
the reader.&lt;/p&gt;

&lt;p&gt;Next we need to add the texture to our scene.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/655602ee3ed04f35a98a2e4d7a5240a4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then we can look up the color based on the texture coordinates.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/f7aa43da0d4a535085b3af2b01538224.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/textured-objects.png&#34; alt=&#34;Textured Objects&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;reflection&#34;&gt;Reflection&lt;/h2&gt;

&lt;p&gt;Conceptually, implementing reflection in a raytracer is quite simple. When a ray
hits a reflective object, just trace another ray out from the intersection at
the appropriate angle - recursively if necessary - and mix the color value from
that ray in with the color value of the first ray.&lt;/p&gt;

&lt;p&gt;As usual, the first thing to do is extend the scene definition. Since the
reflection process is recursive, we also add a value for the maximum recursion
depth. Deeper recursion will produce a more accurate image, but at the cost of
increased rendering time.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/0ff69bf512f8f62ce59d09ec79f222ad.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The reflectivity controls how much of the final pixel color comes from the
reflection and how much from the object itself. If the reflectivity is zero,
we&amp;rsquo;ll use the diffuse color and there will be no reflection. If the reflectivity
is one, we&amp;rsquo;ll use the reflected color and the object will appear to be a
perfect mirror. If the value is somewhere in between, we could get effects
ranging from &amp;lsquo;glossy surface&amp;rsquo; to &amp;lsquo;tinted chrome.&amp;rsquo;&lt;/p&gt;

&lt;p&gt;Since the last part, I&amp;rsquo;ve extracted most of what was in get_color into a function
for doing diffuse shading so that we can use get_color for mixing together the
reflection and diffuse colors.&lt;/p&gt;

&lt;p&gt;As you can see, we construct a reflection ray and trace it through the scene
like with our prime ray, then mix it in with the diffuse color. We also track
the current recursion depth and simply return black if we reach the limit.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/ed31642dcd5a3b58ea1912348e5ae70c.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The more interesting question here is how to compute the reflection ray. If
you&amp;rsquo;ve taken physics, you may remember the mantra that the angle of incidence
equals the angle of reflection. That&amp;rsquo;s helpful enough as far as it goes, but
how do we actually calculate that in terms of vectors?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/reflection-ray.png&#34; alt=&#34;Reflection Ray&#34; align=&#34;right&#34;&gt;&lt;/p&gt;

&lt;p&gt;We can separate the incident vector I into two vectors, A and B (see figure)
such that I = A + B. The reflection vector R is then equal to A - B.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I = A + B
R = A - B
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can compute B quite easily - it&amp;rsquo;s the projection of I onto the surface normal,
or the dot product of I and N multiplied by N.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;B = (I.N)N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Substitute that into both equations:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I = A + (I.N)N
R = A - (I.N)N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then solve the first equation for A:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A = I - (I.N)N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And substitute into the second equation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;R = I - (I.N)N - (I.N)N
R = I - 2(I.N)N
&lt;/code&gt;&lt;/pre&gt;

&lt;script src=&#34;//gist.github.com/bheisler/3a29db0b3104d4420e80523268f3607a.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;We also adjust the origin slightly along the surface normal to avoid the same
floating-point precision problems we had with our shadows earlier.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://imgur.com/a/Kuwks&#34;&gt;&lt;img src=&#34;https://bheisler.github.io/static/reflective-objects.png&#34; alt=&#34;Reflective Objects&#34; /&gt;&lt;/a&gt;
Click to see high-resolution image. Note the recursive reflections between the
center sphere and the floor.&lt;/p&gt;

&lt;h2 id=&#34;refraction&#34;&gt;Refraction&lt;/h2&gt;

&lt;p&gt;Refraction is again conceptually simple in a raytracer - trace a secondary ray
(called the transmission ray) through the object in the appropriate direction
and mix it in with the color of the object. Unfortunately, the math to construct
the transmission ray is a lot more complex than it is to construct the
reflection ray. But first, some definitions:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/04fc2c624e4c3cfae1171a2660a16c2a.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The transparency is the same as the reflectivity from before - the fraction of
the final color that comes from refraction. Refraction is governed by a parameter
called the index of refraction. When a ray of light passes from one transparent
substance to another, it bends at an angle described by Snell&amp;rsquo;s Law:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/Snells_law2.svg&#34; alt=&#34;Snell&#39;s Law&#34; align=&#34;right&#34;&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sin(theta_i)/sin(theta_t) = eta_t/eta_i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where theta_i and theta_t are the angle of incidence and angle of transmission,
and eta_i and eta_t are the indices of refraction for the incident substance and
the transmitting substance. We could calculate the angle of transmission using
this equation, but we&amp;rsquo;ll need to do more to convert that angle into a vector.&lt;/p&gt;

&lt;p&gt;As with reflection, refraction is really a two-dimensional process in the plane
formed by the incident vector and the surface normal. This means that we can
think of our transmitted ray as having a horizontal component (A) and vertical
component (B). B is relatively simple to calculate:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;B = cos(theta_t) * -N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This makes some intuitive sense - the transmitted ray will be on the opposite
side of the surface from the incident ray, so it&amp;rsquo;s vertical component will be
some fraction of the inverse of the surface normal. We use the cosine of the
transmission angle because that&amp;rsquo;s how you calculate the vertical distance.&lt;/p&gt;

&lt;p&gt;We can use this same approach to get the horizontal component A, but first we
need to construct a horizontal unit vector (M). To do this, we first take the
incident vector and subtract it&amp;rsquo;s vertical component, leaving only the
horizontal component. We can calculate the vertical component of I easily -
it&amp;rsquo;s (I.N)N, just like before. Then we normalize this horizontal vector to get
the horizontal unit vector we need. We can slightly cheat here, though - the
length of the horizontal component of I will be equal to sin(theta_i), so we
can normalize using that instead of computing the vector length the slow way.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;M = (I - -N(I.N)) / sin(theta_i) = (I + N(I.N)) / sin(theta_i)
A = sin(theta_t) * M
B = cos(theta_t) * -N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all back together, we get:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;T = A + B
T = (sin(theta_t) * M) - N * cos(theta_t)
T = (sin(theta_t) * (I + N(I.N)) / sin(theta_i)) - N * cos(theta_t)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use Snell&amp;rsquo;s Law to replace that sin(theta_t) / sin(theta_i) with
eta_i/eta_t, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;T = (I + N(I.N)) * eta_i/eta_t - N * cos(theta_t)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We could calculate cos(theta_t) from Snell&amp;rsquo;s Law and theta_i, but this involves
lots of trigonometry, and ain&amp;rsquo;t nobody got time for that. Instead, we can
express that in terms of a dot-product. We know from trigonometry that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cos^2(theta_t) + sin^2(theta_t) = 1
cos(theta_t) = sqrt(1 - sin^2(theta_t))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And from Snell&amp;rsquo;s Law we know that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sin(theta_t) = (eta_i/eta_t) * sin(theta_i)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Therefore:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cos(theta_t) = sqrt( 1 - (eta_i/eta_t)^2 * sin^2(theta_1) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can use the same trigonometric identity from above to convert that sin
to a cosine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cos(theta_t) = sqrt( 1 - (eta_i/eta_t)^2 * (1 - cos^2(theta_i)) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And since cos(theta_i) = I.N, we get:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cos(theta_t) = sqrt( 1 - (eta_i/eta_t)^2 * (1 - I.N^2) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And so, finally, we arrive at this monster of an equation (but look, no trigonometry):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;T = (I + N(I.N)) * eta_i/eta_t - N * sqrt( 1 - (eta_i/eta_t)^2 * (1 - I.N^2) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, there are a couple of wrinkles left to sort out. First, sometimes our ray
will be leaving the transparent object rather than entering it. This is easy
enough to handle, just invert the normal and swap the indices of refraction. We
also need to handle total internal reflection. In some cases, if the angle of
incidence is shallow enough, the refracted light ray actually reflects off the
surface instead of passing through and travels back into the object. We can
detect this when the term inside the sqrt is negative. Again, this makes
intuitive sense - if that&amp;rsquo;s negative, the vertical component of the transmission
vector would be positive (remember, B is a multiple of -N) and therefore on the
same side of the surface as the incident vector. In fact, however, we can handle
this by completely ignoring it, and I&amp;rsquo;ll explain why later.&lt;/p&gt;

&lt;p&gt;Whew! Now that we have that giant equation, we can implement it in code, like so:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/ceba1d949b9b766a03d275534f016fd3.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;I also discovered a nasty bug in my sphere-intersection code while testing this.
If you&amp;rsquo;re following along at home, this could be a good opportunity to practice
your debugging. Go ahead, I&amp;rsquo;ll wait.&lt;/p&gt;

&lt;p&gt;Hint: What happens if the ray origin is inside the sphere?&lt;/p&gt;

&lt;p&gt;Find it? It turns out that the sphere-intersection test will return a point
behind the origin of the ray if the origin is inside the sphere. The refraction
ray will then intersect the sphere again, creating another refraction ray and
so on until we hit the recursion limit. This took hours of painful debugging to
find because I was looking for bugs in the create_transmission function. I
didn&amp;rsquo;t realize that it was something else until I tried to create a refractive
plane and noticed that it appeared to behave correctly.&lt;/p&gt;

&lt;p&gt;Anyway, here&amp;rsquo;s the corrected sphere-intersection function:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/393169985793f91cdd71b5616faace99.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;http://imgur.com/a/T9F6O&#34;&gt;&lt;img src=&#34;https://bheisler.github.io/static/initial-refraction.png&#34; alt=&#34;Initial Refraction&#34; /&gt;&lt;/a&gt;
Click to see high-resolution image. Notice the refracted image of the floor in
the transparent sphere.&lt;/p&gt;

&lt;h2 id=&#34;fresnel&#34;&gt;Fresnel&lt;/h2&gt;

&lt;p&gt;However, we&amp;rsquo;re not quite done yet. If you&amp;rsquo;ve ever noticed how glass buildings or
smooth lakes look like mirrors far away but clear up close, you know that
transparent surfaces reflect light as well as transmitting it. It&amp;rsquo;s often even
possible to see this effect in the polished floors of long hallways. These
reflections are governed by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fresnel_equations&#34;&gt;Fresnel Equations&lt;/a&gt;
and we&amp;rsquo;ll have to simulate them to render refractive objects more accurately.
Incidentally, this is why we can ignore total internal reflection
in our transmission code above - the Fresnel code will cover that for us.
We already know how to handle reflection in our code, but we need to calculate
how much of a ray&amp;rsquo;s color comes from the refraction and how much from the
reflection.&lt;/p&gt;

&lt;p&gt;The derivation of the Fresnel Equations is hairy enough that Scratchapixel
doesn&amp;rsquo;t even try to explain it. Serious physics-lovers might want to check out
&lt;a href=&#34;http://physics.gmu.edu/~ellswort/p263/feqn.pdf&#34;&gt;this derivation&lt;/a&gt; (PDF), but
this is getting out of my depth so I&amp;rsquo;ll just take the final equations as given.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/fresnel-equations.png&#34; alt=&#34;Fresnel Equations&#34; align=&#34;center&#34;&gt;&lt;/p&gt;

&lt;p&gt;Fortunately for me, Scratchapixel does include some C++ code implementing these
equations that I can simply translate to Rust:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/767895b07240a346bcf4353826d269f3.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;And now that we have that, we can put it all together:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2d072e000863e30c7337d3588ff81291.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;http://imgur.com/EvmGhQW&#34;&gt;&lt;img src=&#34;https://bheisler.github.io/static/complete-refraction.png&#34; alt=&#34;Complete Refraction&#34; /&gt;&lt;/a&gt;
Click to see high-resolution image&lt;/p&gt;

&lt;p&gt;Beautiful, isn&amp;rsquo;t it?&lt;/p&gt;

&lt;h2 id=&#34;addendum-gamma-correction&#34;&gt;Addendum - Gamma Correction&lt;/h2&gt;

&lt;p&gt;After I posted this article, &lt;a href=&#34;https://github.com/fstirlitz&#34;&gt;fstirlitz&lt;/a&gt;
&lt;a href=&#34;https://github.com/bheisler/raytracer/issues/2&#34;&gt;pointed out&lt;/a&gt; that the lighting
was a bit off because my code wasn&amp;rsquo;t handling Gamma Correction correctly (or at
all). I had noticed the strange lighting effects but had simply assumed they
were an artifact of my relatively simple rendering process, and that a more
advanced renderer would solve them.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll spare you a detailed discussion of what Gamma Correction is (if you&amp;rsquo;re
interested, see &lt;a href=&#34;http://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/&#34;&gt;What Every Coder Should Know About Gamma&lt;/a&gt; by John Novak).
The short version is that human perception of light is non-linear. Our screens
account for that by applying a power-law formula to the pixel values before
displaying them. Correspondingly, image formats expect the pixel values to be
in this non-linear color space, where my raytracer was writing out pixels in a
linear color space. Likewise, when reading textures, it was reading non-linear
color values and treating them as if they were linear. This mismatch caused
the image to be generally darker, with sharper divisions between light and dark
shades than the human eye would have seen.&lt;/p&gt;

&lt;p&gt;Fortunately, this is pretty easy to fix:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/972da2540646d4195b23a76db9fdd3ac.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;I&amp;rsquo;ve only implemented the basic power-law formula rather than the more complex
sRGB conversion, but it&amp;rsquo;s good enough for now.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://imgur.com/a/tNhlk&#34;&gt;&lt;img src=&#34;https://bheisler.github.io/static/gamma-corrected.png&#34; alt=&#34;Gamma Corrected&#34; /&gt;&lt;/a&gt;
Click to see high-resolution image&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is the end of my series on raytracing, at least for now. There are many,
many things I didn&amp;rsquo;t even begin to cover here. For instance, you might notice
how the two lights in this scene don&amp;rsquo;t glint off the reflective objects the
way real lights would, and how the glass sphere on the right doesn&amp;rsquo;t focus
light rays on the floor like real glass would. If you&amp;rsquo;re interested, I encourage
you to dig deeper - I may return to this subject myself in the future. Until
then, I hope you&amp;rsquo;ve enjoyed reading.&lt;/p&gt;

&lt;p&gt;As before, if you want to try playing around with the code yourself, you can
check out the &lt;a href=&#34;https://github.com/bheisler/raytracer&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing a Raytracer in Rust - Part 2 - Light and Shadow</title>
      <link>https://bheisler.github.io/post/writing-raytracer-in-rust-part-2/</link>
      <pubDate>Mon, 20 Mar 2017 00:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/writing-raytracer-in-rust-part-2/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Welcome to Part 2 of my series on writing a raytracer in Rust. If you haven&amp;rsquo;t
already, you may wish to read &lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/&#34;&gt;Part 1&lt;/a&gt;.
Previously, we implemented a basic raytracer which can render only a single
sphere with no lighting. This time, we&amp;rsquo;ll add multiple objects, planes, and
basic lighting.&lt;/p&gt;

&lt;h2 id=&#34;multiple-objects&#34;&gt;Multiple Objects&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s pretty easy to change our scene definition to contain a Vec of spheres
instead of just a single one. Once we have multiple spheres, however, we need to
know which one our ray hit. This is easy if they don&amp;rsquo;t overlap on the screen.
If they do, we can find the correct sphere by taking the nearest intersection
to our camera. That means we need to know the distance to the intersection, not
just whether there is an intersection or not.&lt;/p&gt;

&lt;p&gt;This requires a bit more geometry. Recall from last time that we detect an
intersection by constructing a right triangle between the camera origin and the
center of the sphere. We can calculate the distance between the center of the
sphere and the camera, and the distance between the camera and the right angle
of our triangle. From there, we can use Pythagoras&amp;rsquo; Theorem to calculate the
length of the opposite side of the triangle. If the length is greater than the
radius of the sphere, there is no intersection.&lt;/p&gt;

&lt;p&gt;There are more right triangles formed by the ray than just this one, however.
If we instead create a triangle between the point that the ray intersects the
sphere and the center of the sphere, we can again use Pythagoras&amp;rsquo; Theorem to
calculate the distance from our right angle to the intersection point.
Subtracting that from the distance from the camera to the right angle gives the
distance to the intersection point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/intersection-distance.png&#34; alt=&#34;Intersection Distance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To put that in code, here are the changes to our Sphere intersection method:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/27f0fc17de209662a04cd6393a8731c3.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Now that we know the distance to the intersection, we need a method to perform
the iteration and return the nearest intersection. It&amp;rsquo;s also useful to return
a reference to the object itself (for example, so we can use the right color).
Notice that we have to use partial_cmp and unwrap to compare the distances. This
is an instance where Rust&amp;rsquo;s strict type safety sort of gets in the way - because
some values (NaN, +-Infinity) can&amp;rsquo;t be correctly compared, f64 doesn&amp;rsquo;t implement
the Cmp trait. In this case, no valid intersection can ever contain those values
so we should be safe just using unwrap. It&amp;rsquo;s a bit ugly, but
it&amp;rsquo;s probably better than tracking down strange bugs related to NaN-safety later.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/95ffb04905984a10fe00580851e19380.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;planes&#34;&gt;Planes&lt;/h2&gt;

&lt;p&gt;Next up we&amp;rsquo;ll add Planes as an object to test our rays against. There are a few
ways to represent planes in 3D space, but for our purposes the most convenient
is to define a point on the plane, and the normal of the surface. Before we
implement the intersection test though, we need to adapt our scene structure
so it can contain an arbitrary number of spheres or planes.&lt;/p&gt;

&lt;p&gt;We could try adding another Vec of Plane structures, but that gets annoying
quickly. We&amp;rsquo;d have to duplicate some logic (eg. the trace method) to apply to
both the spheres and the planes. Some sort of dynamic dispatch is appropriate
here. Rust provides two ways to do this. We could either wrap each object in a
variant of an Enum or we could use
&lt;a href=&#34;https://doc.rust-lang.org/book/trait-objects.html&#34;&gt;Trait Objects&lt;/a&gt;. I&amp;rsquo;ve chosen
to go with the former, but it&amp;rsquo;s mostly a matter of personal preference.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/7c4f1d7580a79c636d8b1ddfaecd652f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Now that we have our Plane structure, how can we test for an intersection?
One convenient property of planes is that they&amp;rsquo;re infinitely large. If a plane
isn&amp;rsquo;t perfectly parallel to our ray, it will always intersect eventually.
We can test this with the dot product - if the dot product between the ray and
the normal of the plane is zero (give or take a bit to account for
floating-point error) then it&amp;rsquo;s parallel and thus there is no intersection.
Otherwise, there is an intersection somewhere.&lt;/p&gt;

&lt;p&gt;However, we need to know where that intersection is. I&amp;rsquo;m afraid that I haven&amp;rsquo;t
been able to find a good intuitive or geometric explanation of why this works,
so I&amp;rsquo;ll just have to direct you to
&lt;a href=&#34;https://www.scratchapixel.com/lessons/3d-basic-rendering/minimal-ray-tracer-rendering-simple-shapes/ray-plane-and-ray-disk-intersection&#34;&gt;Scratchapixel&lt;/a&gt;,
where they show the full derivation of the equation.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/388e88f4cb2ecb391c63aabd76859c5f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Now that all of that&amp;rsquo;s done, let&amp;rsquo;s take a moment to admire our handywork.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/spheres-and-planes.png&#34; alt=&#34;Spheres and Planes&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Yeah, I know. Five minutes in MS Paint, amirite? It will look better once we
start adding lighting effects.&lt;/p&gt;

&lt;h2 id=&#34;directional-lights&#34;&gt;Directional Lights&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll start by adding a single directional light to our scene. Directional lights
approximate light from the sun or stars - objects so far away that their light
rays are effectively parallel to each other and at an approximately-constant
intensity level. As a result, they&amp;rsquo;re also simpler than closer point-source
lights.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/5884657ad43bb968c8dbcedddfe72faf.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Next we need to know the surface normal of the object at the point our ray
intersected with it.&lt;/p&gt;

&lt;p&gt;Sphere:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fn surface_normal(&amp;amp;self, hit_point: &amp;amp;Point) -&amp;gt; Vector3 {
    (*hit_point - self.center).normalize()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plane:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fn surface_normal(&amp;amp;self, _: &amp;amp;Point) -&amp;gt; Vector3 {
    -self.normal
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we&amp;rsquo;ll need to add an albedo to our Spheres and Planes. This is simply
a parameter which specifies how much light energy is reflected by an object and
how much is absorbed.&lt;/p&gt;

&lt;p&gt;Now to actually implement the shading. First some preparation.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let intersection = scene.trace(&amp;amp;ray);
let hit_point = ray.origin + (ray.direction * intersection.distance)
let surface_normal = intersection.element.surface_normal(&amp;amp;hit_point)
let direction_to_light = -scene.light.direction
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we calculate the amount of light that lands on this point. This is
proportional to the cosine of the angle between the surface normal and the
direction to the light (&lt;a href=&#34;https://en.wikipedia.org/wiki/Lambert%27s_cosine_law&#34;&gt;Lambert&amp;rsquo;s Cosine Law&lt;/a&gt;).
The dot product is the length of one vector times the
cosine of the angle between them, but because we use normalized vectors the
length will be one. We also add a factor for the brightness of the light.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let light_power = (surface_normal.dot(&amp;amp;direction_to_light) as f32) *
    scene.light.intensity;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we calculate the proportion of the light which is reflected. This is equal
to the albedo of the object divided by Pi. Once again I have to admit that I
can&amp;rsquo;t find a good explanation of this formula. If you&amp;rsquo;re really interested, you
can once again check out Scratchapixel&amp;rsquo;s
&lt;a href=&#34;https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/diffuse-lambertian-shading&#34;&gt;derivation&lt;/a&gt;
(be warned - this one contains integrals). The short version is that dividing by
Pi ensures that the object doesn&amp;rsquo;t reflect away more energy than it receives.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let light_reflected = intersection.element.albedo() / std::f32::consts::PI;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we accumulate this together into the final color for the pixel.
We represent colors as (R, G, B) triplets where each value is in the range
0.0&amp;hellip;1.0. We can multiply colors by multiplying each value - eg. if the red
component of a light is 0.5 and the object reflects 0.5 of red light, the viewer
will receive a red value of 0.25.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let color = intersection.element.color() * scene.light.color *
            light_power * light_reflected;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or, all together:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/7fe4960b607344aa57a06d4712685ab5.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/directional-lighting.png&#34; alt=&#34;Directional Lighting&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s still not quite right though - none of the spheres are casting shadows, on
the lower plane or on each other.&lt;/p&gt;

&lt;h2 id=&#34;shadows&#34;&gt;Shadows&lt;/h2&gt;

&lt;p&gt;Calculating shadows in a raytracer is really easy. Simply trace another ray from
the intersection of the prime ray and the object back to the light. If there is
another object between the intersection and the light, the point is in shadow.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/14fae787f7092ad068a572d9b406d10f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/shadow-acne.png&#34; alt=&#34;Shadow Acne&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Well, that&amp;rsquo;s not quite right. We have shadows on the lower plane and the green
sphere, but also a lot of noise. The dark noise is called &amp;lsquo;shadow acne&amp;rsquo; and
it occurs because our floating point values have limited precision. Sometimes,
the hit point will be ever so slightly inside the intersected object and so the
shadow ray will intersect with the same object the prime ray did. It might seem
like we could simply ignore that object when tracing the shadow ray, and for
this simple geometry we could. If we had more complex objects though (eg. a model of
a tree) we would want an object to be able to cast shadows on itself, so that
won&amp;rsquo;t work. Instead, we simply add a tiny fudge factor and adjust the origin
of the shadow ray a short distance along the surface normal so that we can be
sure it&amp;rsquo;s outside the object. It doesn&amp;rsquo;t have to be much - I&amp;rsquo;ve found that bias
values as small as 1e-13 were enough to eliminate visible shadow acne.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let shadow_ray = Ray {
    origin: hit_point + (surface_normal * scene.shadow_bias),
    direction: direction_to_light,
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/shadows.png&#34; alt=&#34;Shadows&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;multiple-lights&#34;&gt;Multiple Lights&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s pretty easy to implement multiple lights as well. The light the camera sees
from any particular point is equal to the sum of the contributions from each
individual light source. We can just iterate through the lights, accumulating
together the color values from each.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/c6186ef183ed98fccd02c119c2cc01a4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;This produces the following image - notice the two sets of shadows.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/multiple-lights.png&#34; alt=&#34;Multiple Lights&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;spherical-lights&#34;&gt;Spherical Lights&lt;/h2&gt;

&lt;p&gt;Finally, we&amp;rsquo;ll add Spherical Lights (or point lights). First some definitions.
Again, I&amp;rsquo;m using an enum for dynamic dispatch.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2aad485bfef7087438c493e3ca6a5bdc.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Next up, we need to know the direction to the light. This is easily calculated:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(s.position - *hit_point).normalize()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The intensity of these lights obeys the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Inverse-square_law&#34;&gt;Inverse Square Law&lt;/a&gt;, so we
calculate the intensity by dividing the light&amp;rsquo;s intensity value by 4*Pi*distance^2.
Incidentally, this means that the intensity values of spherical lights in your
scene definition must be much larger than for directional lights.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let r2 = (s.position - *hit_point).norm() as f32;
s.intensity / (4.0 * ::std::f32::consts::PI * r2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Additionally, our shadow test needs to be changed a bit. For directional lights,
we only had to check if there was any intersection in the direction of the light.
That won&amp;rsquo;t work now - what if there&amp;rsquo;s an object on the far side of the light?
Instead we check if the nearest intersection is closer than the light itself is.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let shadow_intersection = scene.trace(&amp;amp;shadow_ray);
let in_light = shadow_intersection.is_none() ||
    shadow_intersection.unwrap().distance &amp;gt; light.distance(&amp;amp;hit_point);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting that all together produces this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/spherical-lights.png&#34; alt=&#34;Spherical Lights&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Try doing that in five minutes in MS Paint!&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve taken this toy raytracer from producing an image of a green circle to a
nicely-lit scene containing multiple objects. The &lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-3/&#34;&gt;last entry&lt;/a&gt;
in this series will go on to add texturing as well as simple reflection and
refraction simulations. As before, if you want to try playing around with the
code yourself, you can check out the
&lt;a href=&#34;https://github.com/bheisler/raytracer&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing a Raytracer in Rust - Part 1 - First Rays</title>
      <link>https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/</link>
      <pubDate>Mon, 20 Feb 2017 11:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Hello! This is part one of a short series of posts on writing a simple raytracer
in Rust. I&amp;rsquo;ve never written one of these before, so it should be a learning
experience all around.&lt;/p&gt;

&lt;p&gt;So what is a raytracer anyway? The short version is it&amp;rsquo;s a computer program that
traces the paths of simulated rays of light through a scene to produce
high-quality 3D-rendered images. Despite that, it also happens to be the simplest
way to render 3D images. Unfortunately, that comes at a cost in render time -
raytracing an image takes much longer than the polygon-based rendering done by
most game engines. This means that raytracing is typically used to produce
&lt;a href=&#34;http://hof.povray.org/&#34;&gt;beautiful still images&lt;/a&gt; or pre-rendered video (eg.
Pixar&amp;rsquo;s &lt;a href=&#34;https://renderman.pixar.com/&#34;&gt;RenderMan&lt;/a&gt; technology).&lt;/p&gt;

&lt;p&gt;For the purposes of this post, I&amp;rsquo;ll assume that you&amp;rsquo;re familiar with what
vectors are and how they work, as well as basic geometry. If you aren&amp;rsquo;t, check
out the first three pages of Scratchapixel&amp;rsquo;s
&lt;a href=&#34;https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/points-vectors-and-normals&#34;&gt;excellent series&lt;/a&gt;
on geometry and linear algebra. You don&amp;rsquo;t need to know Rust specifically (though
I recommend it, it&amp;rsquo;s a great language) but you should at least be familiar with
C-family programming languages. If you want to build the code however, you will
need to install &lt;a href=&#34;https://rustup.rs/&#34;&gt;Cargo&lt;/a&gt;, which is the standard Rust build
tool.&lt;/p&gt;

&lt;h2 id=&#34;defining-the-scene&#34;&gt;Defining the Scene&lt;/h2&gt;

&lt;p&gt;The first thing to do is decide exactly what our scene (and therefore our
renderer) will be able to handle. For this first post, it won&amp;rsquo;t be much. One
lonely sphere, hanging in the darkness. No lighting, reflection, or transparency.
No other shapes. We&amp;rsquo;ll extend this basic scene over the rest of this series.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start by defining some structures to hold our scene data:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/bf4247cf7921d8c449e3cd62f323519d.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;And a stub render method and simple test case. I&amp;rsquo;m using the
&lt;a href=&#34;https://crates.io/crates/image&#34;&gt;image crate&lt;/a&gt; to set up the image buffer and
write the resulting render to a PNG file. This is all pretty straightforward
except for the position of the sphere - (0.0, 0.0, -5.0). I&amp;rsquo;ll explain that
later.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/1eb2e5fadc9edb680760360ee53f9a78.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;prime-ray-generation&#34;&gt;Prime Ray Generation&lt;/h2&gt;

&lt;p&gt;The basic idea of how a raytracer like this works is that we iterate over every
pixel in the finished image, then trace a ray from the camera out through that
pixel to see what it hits. This is the exact opposite of how real light works,
but it amounts to pretty much the same thing in the end. Rays traced from the
camera are known as prime rays or camera rays. There is actually a lot of
freedom in how we translate pixel coordinates to prime rays, which confused me
for a while, but it&amp;rsquo;s pretty simple if you follow common conventions.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start with defining a Ray structure and a static function for generating
prime rays:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/e8f47c4cad5b1210231d66200846f653.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;By convention, the camera is aligned along the negative z-axis, with positive x
towards the right and positive y being up. That&amp;rsquo;s why the sphere is at
(0.0, 0.0, -5.0) - it&amp;rsquo;s directly centered, five units away from the camera.
We&amp;rsquo;ll start by pretending there&amp;rsquo;s a two-unit by two-unit square one unit in
front of the camera. This square represents the image sensor or film of our camera.
Then we&amp;rsquo;ll divide that sensor square into pixels, and use the directions to each
pixel as our rays. We need to translate the (0&amp;hellip;800, 0&amp;hellip;600) coordinates of our
pixels to the (-1.0&amp;hellip;1.0, -1.0&amp;hellip;1.0) coordinates of the sensor. I&amp;rsquo;ll start
with the finished code for this step, then explain it in more detail.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/be79c6e0871e4308443c0d4e61318fed.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Let&amp;rsquo;s unpack that a bit and focus on only the x component. The y component is
almost exactly the same.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let pixel_center = x as f64 + 0.5;
let normalized_to_width = pixel_center / screen.width as f64;
let adjusted_screen_pos = (normalized_to_width * 2.0) - 1.0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, we cast to float and add 0.5 (one half-pixel) because we want our ray to
pass through the center (rather than the corner) of the pixel on our imaginary
sensor. Then we divide by the image width to convert from our original
coordinates (0&amp;hellip;800) to (0.0&amp;hellip;1.0). That&amp;rsquo;s almost, but not quite, the
(-1.0&amp;hellip;1.0) coordinates we want, so we multiply by two and subtract one. That&amp;rsquo;s
all there is to it! The y calculation follows the same basic process except the
last step:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let adjusted_screen_pos = 1.0 - (normalized_to_width * 2.0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is simply because the image coordinates have positive y meaning down, where
we want positive y to be up. To correct for this, we simply take the negative of
the last step of the calculation.&lt;/p&gt;

&lt;p&gt;Then we pack the x and y components into a vector (z is -1.0 because all
of our prime rays should go forward from the camera) and normalize it to get a
nice direction vector. Simple, right? This is why the 2x2 sensor 1 unit from the
camera convention is convenient. If we&amp;rsquo;d used any other set of coordinates than
(-1.0&amp;hellip;1.0, -1.0&amp;hellip;1.0) then the image would be off center and/or we&amp;rsquo;d have to
do more calculations to avoid distorting it.&lt;/p&gt;

&lt;p&gt;We could actually stop here - this is a working prime ray generation function.
However, it assumes that the image we&amp;rsquo;re generating is perfectly square and that
the field of view is precisely 90 degrees. It&amp;rsquo;s probably worth adding a
correction for other aspect ratios and different fields of view.&lt;/p&gt;

&lt;p&gt;To adjust for different aspect ratios, we calculate the aspect ratio and
multiply it by the x coordinate. We&amp;rsquo;re assuming that the image will be wider than
it is tall, but most images are so that&amp;rsquo;s good enough for now. If we didn&amp;rsquo;t do
this, the rays would be closer together in the x direction than in the y, which
would cause a distortion in the image (where every pixel is the same size in
both directions).&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/1bef7641a1ce2e52957f65a9022e6a0f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then we can add another adjustment for field of view. Field of view is the angle
between the left-most ray and the right-most ray (or top- and bottom-most). We
can use simple trigonometry to calculate how much we need to adjust the
coordinates by:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/477ad79cdb635cee87f6e7672d1bc3dc.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;You might have noticed that the origin of all prime rays is exactly (0, 0, 0).
This means that our camera is fixed at those coordinates. It is possible to adapt
this function to place the camera in different locations or orientations, but
we won&amp;rsquo;t need that for now.&lt;/p&gt;

&lt;h2 id=&#34;testing-for-intersections-with-the-sphere&#34;&gt;Testing for Intersections With The Sphere&lt;/h2&gt;

&lt;p&gt;Now that we have our prime rays, we need to know if they intersect with our
sphere. As usual, we&amp;rsquo;ll start with some definitions.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/5cb206e9d4f0dda63a44c1fa5d2908a2.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The basic idea behind this test is that we construct a right-triangle using the
prime ray as the adjacent side and the line between the origin and the center
of the sphere as the hypotenuse. Then we calculate the length of the opposite
side using the Pythagorean Theorem - if that side is smaller than the radius of
the sphere, the ray must intersect the sphere. In practice, we actually do the
check on length-squared values because square roots are expensive to calculate,
but it&amp;rsquo;s the same idea.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/sphere-intersection-test.png&#34; alt=&#34;Sphere Intersection Test&#34; /&gt;&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2fd3e237481614d13a34dc184cb5d106.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;finishing-the-render-method&#34;&gt;Finishing the Render Method&lt;/h2&gt;

&lt;p&gt;Now that we have all of the hard parts done, we simply need to integrate these
functions into the render function and produce our image:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/b2f715736405503985ef66f3732746c5.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;After adding some extra glue code to parse a scene definition and save the
rendered image to a file, we get the resulting image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/raytracer-first-render.png&#34; alt=&#34;First Rendered Image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It isn&amp;rsquo;t very impressive yet, but we&amp;rsquo;ll add more detail to it as we go. In the
&lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-2/&#34;&gt;next post&lt;/a&gt;,
we&amp;rsquo;ll add planes, multiple spheres, and some basic lighting effects.&lt;/p&gt;

&lt;p&gt;If you want to try playing around with the code yourself, you can check out the
&lt;a href=&#34;https://github.com/bheisler/raytracer&#34;&gt;GitHub Repository&lt;/a&gt;. If you want to learn
more about 3D rendering in general or raytracing in particular, check out
&lt;a href=&#34;https://www.scratchapixel.com/index.php&#34;&gt;Scratchapixel&lt;/a&gt;, which is the resource
I used while working on this.&lt;/p&gt;

&lt;p&gt;Thanks to Scott Olson and Daniel Hogan for suggesting improvements to an
earlier version of this article.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>