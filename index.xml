<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bheisler.github.io</title>
    <link>https://bheisler.github.io/index.xml</link>
    <description>Recent content on bheisler.github.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 30 Jul 2017 02:11:19 +0000</lastBuildDate>
    <atom:link href="https://bheisler.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Parsing NES ROM Headers with nom</title>
      <link>https://bheisler.github.io/post/nes-rom-parser-with-nom/</link>
      <pubDate>Sun, 30 Jul 2017 02:11:19 +0000</pubDate>
      
      <guid>https://bheisler.github.io/post/nes-rom-parser-with-nom/</guid>
      <description>

&lt;p&gt;Long, long ago (December 2015) I wanted to learn how emulators worked, so I
decided to write an &lt;a href=&#34;https://github.com/bheisler/Corrosion&#34;&gt;NES emulator&lt;/a&gt;.
Not only that, but I opted to write it in Rust, a language which I had never
used before. A crazy idea, to be certain, but once I was done I had indeed
learned a great deal about emulators, the NES, and Rust.&lt;/p&gt;

&lt;p&gt;Anyway, I&amp;rsquo;ve been working on that project again lately, doing some maintenance work
and upgrades. One of the things I did was rewrite the ROM parser using
&lt;a href=&#34;https://github.com/Geal/nom&#34;&gt;nom&lt;/a&gt;. The ROM parser was the first bit of Rust code
I&amp;rsquo;ve ever written, and it was not great, so I thought it was finally time to clean
it up a bit. This post is a short description of that process and my thoughts on
nom as a newcomer to this library. First, a detour to discuss the NES ROM format
- if you&amp;rsquo;re not interested in the fine details, you can skip ahead to the next
section.&lt;/p&gt;

&lt;h2 id=&#34;the-ines-header&#34;&gt;The iNES Header&lt;/h2&gt;

&lt;p&gt;Nearly all NES ROM files are in one of two formats. There&amp;rsquo;s the
&lt;a href=&#34;https://wiki.nesdev.com/w/index.php/INES&#34;&gt;iNES&lt;/a&gt; format, or a later extension
called &lt;a href=&#34;https://wiki.nesdev.com/w/index.php/NES_2.0&#34;&gt;NES 2.0&lt;/a&gt;. My existing parser
only supports iNES, and that&amp;rsquo;s all the new parser will support as well. I haven&amp;rsquo;t
come across many ROM&amp;rsquo;s in NES 2.0 format yet, so I haven&amp;rsquo;t needed to add support
for it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start digging into the iNES header and I&amp;rsquo;ll explain what everything is as
we go. First, we have a four-byte magic number - the ASCII letters &amp;lsquo;NES&amp;rsquo; followed
by 0x1A - the DOS end-of-file character. After that is one byte holding the length
of the PRG ROM data in 16kB blocks and one byte for the length of CHR ROM in 8kB blocks.
NES cartridges contain PRG ROM and CHR ROM. PRG (program) ROM holds the
assembled program code and associated data for the game. It&amp;rsquo;s available in the
main CPU memory map by reading 0x4020 to 0xFFFF, though many cartridges only
support reading PRG ROM at addresses above 0x8000. CHR (character) ROM holds the
graphical data for the game and is only indirectly accessible to the CPU.
CHR ROM is used instead by the PPU (Picture Processing Unit). Some cartridges
have no CHR ROM and instead use CHR RAM, transferring the graphical data
to the CHR RAM at runtime.&lt;/p&gt;

&lt;p&gt;After those two bytes are two more bytes of flags. These include various bits
of information about the hardware of the cartridge (eg. whether or not the
cartridge has battery-backed RAM for saving your game). These flag bytes also
contain the mapper ID for the game. Mappers are one of the more&amp;hellip; interesting&amp;hellip;
aspects of NES emulation. As I mentioned before, the PRG ROM is typically
accessible from 0x8000 to 0xFFFF - a window of 32kB. 32kB is not nearly large
enough for most NES games (some of which have as much as 1MB of PRG ROM alone). To
deal with this, cartridges contain circuit boards called mappers which map
pages of the ROM in and out of the address space. Different games, and especially
games by different manufacturers, often have wildly different mappers. The
emulator must emulate the mapper as well, so the header contains one byte
(split into two 4-bit pieces for historical reasons) containing the mapper ID.&lt;/p&gt;

&lt;p&gt;After the first two flag bytes is another page-count byte, this time for PRG
RAM (the battery-backed save RAM in games like Legend of Zelda) and another
flags byte. Finally, we have six reserved bytes, which are not used by iNES but
are used by NES 2.0.&lt;/p&gt;

&lt;p&gt;To recap:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;4-byte magic number&lt;/li&gt;
&lt;li&gt;PRG ROM page count&lt;/li&gt;
&lt;li&gt;CHR ROM page count&lt;/li&gt;
&lt;li&gt;Lower half of mapper number &amp;amp; flags&lt;/li&gt;
&lt;li&gt;Upper half of mapper number &amp;amp; flags&lt;/li&gt;
&lt;li&gt;More flags&lt;/li&gt;
&lt;li&gt;Six bytes of zeroes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following this header is an optional 512-byte trainer (extra program code added
by some ROM-ripping devices), and the actual PRG and CHR ROM data. Now that we know
what we&amp;rsquo;re parsing, let&amp;rsquo;s take a look at nom.&lt;/p&gt;

&lt;h2 id=&#34;nom&#34;&gt;nom&lt;/h2&gt;

&lt;p&gt;The way parsing works in nom is you use the do_parse! macro to define your
parser, and a number of other functions and macros to define the structure of
your data. These macros and functions collectively generate some Rust code which
parses that data and returns one of three possible results - Done (containing any
remaining, unparsed data and the resulting value), Incomplete (meaning more
data is needed) or Error (meaning the data is invalid or otherwise couldn&amp;rsquo;t
be parsed). The use of macros for this is a rather clever idea, though not
without downsides.&lt;/p&gt;

&lt;p&gt;One of those downsides is that the compiler can&amp;rsquo;t really help you when you
make a mistake. For instance, it took me longer than I&amp;rsquo;d like to admit to get
the following code to compile before I realized that I had forgotten to pass
the input to the do_parse macro.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2fae3b00eedcb80a1a109e622a55f74f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Once I got going though, it was pretty smooth sailing and extremely fast to
parse out the rest of the header and construct my Rom structure. The tag!
macro takes a given sequence of bytes and reads that sequence from the input.
be_u8 (the &amp;lsquo;be&amp;rsquo; means big-endian) is a one-byte unsigned integer. Then we have
the cond! macro, which applies a given parser if some condition is true, and
finally the take! macro, which consumes a given number of bytes and returns
them as a slice.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/a39a0fdd5b1741ce7982849febe914c8.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Since my code doesn&amp;rsquo;t support the NES 2.0 extension, I wanted to detect if a ROM
was using that format and return an error. This is where I started to run into
trouble; I couldn&amp;rsquo;t find an obvious way to conditionally return an error.
I ended up working around it by using the call! macro to call a function I wrote
which would return an error if the ROM was in NES 2.0 format. This was somewhat
surprising to me; this seems like it would be a common problem.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/68c917c9f44934dd848b0efa61e7dbdd.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;At this point, I had a working parser, but I decided to take the opportunity to
rework my code a bit as well. Previously, I simply stored the flag bytes in the
Rom structure and left it to other code to mask out the individual flags, as well
as the two 4-bit pieces of the mapper ID. nom can parse individual bits out of
the input as well, so I started with separating out the mapper ID from the rest
of the flag bytes.&lt;/p&gt;

&lt;p&gt;nom overall could use some work on its documentation, but using the bit-indexing
is particularly opaque. I had to look up a cached version of an old blog post
(&lt;a href=&#34;https://webcache.googleusercontent.com/search?q=cache:4CNayFlPRicJ:siciarz.net/24-days-rust-nom-part-2/+&amp;amp;cd=1&amp;amp;hl=en&amp;amp;ct=clnk&amp;amp;gl=ca&#34;&gt;link&lt;/a&gt;)
to find out how to do it. To spare you the same trouble, here&amp;rsquo;s a quick overview.&lt;/p&gt;

&lt;p&gt;The bits! macro takes a bit-stream parser (eg. take_bits!) or a type-agnostic
parser (eg. tuple!) and generates the code to apply that parser to a byte-slice
input. There is also a bytes! macro to go the other way, applying a byte-slice
parser to a bit-stream input. Inside the bits! macro, you can use parsers that
consume individual bits. When switching from bit-stream to byte-slice parsing
(that is, at the end of the bits! macro or the beginning of a bytes! macro), if
there&amp;rsquo;s a partial byte remaining in the input it will be ignored and the
subsequent byte-slice parser will start parsing at the next whole byte. The only
two built-in bit-stream parsers are take_bits! (which consumes a given number
of bits from the input, and assembles them into the given integer type) and
tag_bits! which is like tag! but for bits.&lt;/p&gt;

&lt;p&gt;Unfortunately, at this point it isn&amp;rsquo;t possible to give names to each value in
a bits! macro like it is in do_parse!, so I had to make do with collecting the
mapper ID bits and the flag bits into a tuple instead.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/30d84186661c04a6d383278a18199e34.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;I went on to make some further changes, but they&amp;rsquo;re not related to nom so I&amp;rsquo;ll
skip the details. You can take a look at the &lt;a href=&#34;https://github.com/bheisler/Corrosion/blob/fdc4fa0334aabaa76518479dd0ad3e62e4e5ebb1/src/cart/ines.rs&#34;&gt;code&lt;/a&gt;
if you&amp;rsquo;re interested.&lt;/p&gt;

&lt;h2 id=&#34;impressions-of-nom&#34;&gt;Impressions of nom&lt;/h2&gt;

&lt;p&gt;I kind of like nom. There&amp;rsquo;s a rocky learning curve, and the documentation needs
some work. I&amp;rsquo;m also a bit wary of such heavy use of macros. Parsing is (often)
not performance-critical, so I&amp;rsquo;d be willing to sacrifice some runtime efficiency
to get some more help from the compiler when I make mistakes. On the other hand,
once you do get the hang of it, it&amp;rsquo;s quick and easy to define parsers for quite
complex data structures and the code reads a lot like a description of the format
to be parsed, which is always nice. nom has some beautifully clear example parsers
to look at (take &lt;a href=&#34;https://github.com/Geal/gif.rs&#34;&gt;this GIF parser&lt;/a&gt;,
for example). It works on both binary and text data as well, which is a plus.&lt;/p&gt;

&lt;p&gt;Overall, I would consider nom for future projects that involve parsing data. The
lack of documentation could cause some headaches, but it&amp;rsquo;s much easier and safer
to use a battle-tested library like nom than it is to write your own hand-written
parser for the same data.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;d like to check out the code or play around with some perfectly legal,
homebrew NES software, you can find it on
&lt;a href=&#34;https://github.com/bheisler/Corrosion&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://bheisler.github.io/about/about/</link>
      <pubDate>Tue, 25 Apr 2017 14:29:02 -0600</pubDate>
      
      <guid>https://bheisler.github.io/about/about/</guid>
      <description>&lt;p&gt;Hello! I&amp;rsquo;m Brook Heisler. I&amp;rsquo;m a software developer from Saskatoon. When I&amp;rsquo;m not
busy programming, I spend my time propmaking instead. This is reflected in the
dual nature of this blog, which is mostly about my hobby projects. Here, you&amp;rsquo;ll
find detailed build logs about my prop projects and even-more-detailed
information on my code projects, as well as whatever else I feel like writing
about. If you&amp;rsquo;re only interested in one of those two things, that&amp;rsquo;s cool - use
the category links above to focus on only code or only props. There are even
separate RSS feeds. Cheers!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calling Rust From Python</title>
      <link>https://bheisler.github.io/post/calling-rust-in-python/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/calling-rust-in-python/</guid>
      <description>

&lt;p&gt;Hello! This is a detailed example of exposing Rust code to other languages (in
this case, Python). Most articles I&amp;rsquo;ve seen that cover this topic uses really
trivial example functions, skipping over a lot of the complexity. Even the better
ones out there typically don&amp;rsquo;t have a pre-existing, reasonably complex program
to work with. I&amp;rsquo;m going to start with trivial functions and build my way up to
being able to define a scene for my &lt;a href=&#34;https://github.com/bheisler/raytracer&#34;&gt;raytracer&lt;/a&gt;
in Python using a series of calls to Rust, then render it and return the
resulting image data back to Python. If you want to know more about the raytracer,
I wrote a series of posts on it &lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/&#34;&gt;here&lt;/a&gt;,
but it won&amp;rsquo;t be necessary; I&amp;rsquo;ll explain parts of the raytracer here as we need
them. Hopefully this will give a more complete picture of how to incorporate
complex Rust code into Python.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve never written any sort of Python/C interop before, so this should be another
learning experience all around. I&amp;rsquo;m going to arbitrarily choose
&lt;a href=&#34;https://cffi.readthedocs.io/en/latest/&#34;&gt;CFFI&lt;/a&gt; as the Python interop library.
It&amp;rsquo;s portable across interpreters and seems nicer to use than &lt;a href=&#34;https://docs.python.org/2/library/ctypes.html&#34;&gt;ctypes&lt;/a&gt;.
I expect the main concepts will be broadly applicable to other libraries (and
other languages such as Ruby). Let get started!&lt;/p&gt;

&lt;h2 id=&#34;calling-functions&#34;&gt;Calling Functions&lt;/h2&gt;

&lt;p&gt;The first thing to do is to define a Rust function we want to call from Python.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/effc8c457c9d85d1e318be52e1b8c98d.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;We&amp;rsquo;re actually defining a function for Rust&amp;rsquo;s C foreign-function interface. The
basic idea here is that we write a wrapper in Python that knows how to call
C functions, and a wrapper in Rust that exposes C functions and translates them
to regular function calls in Rust. It&amp;rsquo;s sort of like we&amp;rsquo;re calling from Python
into C into Rust. The &lt;code&gt;no_mangle&lt;/code&gt; attribute and &lt;code&gt;extern &amp;quot;C&amp;quot;&lt;/code&gt; above instruct rustc
not to change the name of the function (otherwise CFFI wouldn&amp;rsquo;t be able to
find it later) and to emit a function that can be called as if it were written
in C. We&amp;rsquo;ll need both for all functions that we want to expose to C.&lt;/p&gt;

&lt;p&gt;Now we need to instruct Cargo to build this library as a dynamic library
(&amp;ldquo;dylib&amp;rdquo; in Cargo terms). I&amp;rsquo;m writing this on a Windows PC, so Cargo
produces a &lt;code&gt;raytracer_ffi.dll&lt;/code&gt; file. I tested it on Linux as well and it created
&lt;code&gt;libraytracer_ffi.so&lt;/code&gt;.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/06c25b67a35bfd8f5b38781256558230.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then we need some Python code to load and call this shared library:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/ec798db12cd69153a6330e67eb6d3dac.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Let&amp;rsquo;s break this down a bit. First we import the &lt;code&gt;cffi&lt;/code&gt; module and create an
FFI object. Then we call &lt;code&gt;cdef&lt;/code&gt; and pass it some text - this text is a C function
signature matching the &lt;code&gt;double&lt;/code&gt; function in Rust. CFFI parses this function
signature in order to determine how to call the function. We&amp;rsquo;ll need to do this
for all of the functions and structs we want to expose to Python. Then we open
the DLL file with &lt;code&gt;dlopen&lt;/code&gt;. Finally, we call the &lt;code&gt;double&lt;/code&gt; function as if it were
a regular Python function and print the result.&lt;/p&gt;

&lt;p&gt;And now we should be able to call &lt;code&gt;double&lt;/code&gt; from Python:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe test.py
18
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Side note: I wasn&amp;rsquo;t able to get this working with PyPy on 64-bit Windows. I
didn&amp;rsquo;t find out why, but I assume it has something to do with how PyPy only
provides 32-bit binaries. PyPy worked fine for me on Linux, but I had to use
64-bit CPython on Windows.&lt;/p&gt;

&lt;h2 id=&#34;passing-structures&#34;&gt;Passing Structures&lt;/h2&gt;

&lt;p&gt;Now, if I&amp;rsquo;m going to be able to define a scene in Python, I&amp;rsquo;ll need to be able
to call functions and pass in structs as arguments. I&amp;rsquo;ll keep working with this
toy program a bit longer, but instead of simply doubling an integer, let&amp;rsquo;s try
and get it to calculate the length of a vector using &lt;code&gt;vector::Vector3::length&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;First, I&amp;rsquo;ll need to tell rustc that Vector3 should be laid out like a C struct.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/23de3e8f86143ceea2240b2a283b8f91.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;It appears that CFFI doesn&amp;rsquo;t have any way to call functions with stack-allocated
structures. Using the stack for small, copyable structures like Vector3 is
pretty common in Rust, but I guess it isn&amp;rsquo;t in C? So instead, our Rust function
will have to accept a pointer to a Vector3.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/f02545b55a01f5602f9aa8802c970847.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Here we define an extern function which accepts a raw pointer to a Vector3.
Dereferencing raw pointers is unsafe, so we use an unsafe block to convert the
raw pointer to a Rust reference. Finally, we call &lt;code&gt;length()&lt;/code&gt; and return the value.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/d7c3c411f6826303a8a09868821b6829.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Back in Python-land, we define a structure type matching Vector3 and the
signature of the length function. Now we need to allocate a new vector_t object,
which is done with the &lt;code&gt;ffi.new()&lt;/code&gt; function. We need to pay attention to
ownership here - the memory for the vector_t is allocated by Python and it will
have to be freed by Python. In this case, it will be freed when the vector object
gets garbage collected so we don&amp;rsquo;t need to worry about it, but we&amp;rsquo;ll need to
be more careful about ownership later.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe test.py
1.73205080757
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;returning-references-back-to-python&#34;&gt;Returning References Back To Python&lt;/h2&gt;

&lt;p&gt;Now we&amp;rsquo;ll start the process of building our actual FFI code. We&amp;rsquo;ll start with the
Scene structure. I don&amp;rsquo;t especially want to expose all the complexity of the
Scene structure to Python, so instead we&amp;rsquo;ll use another C idiom and return an
opaque pointer.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/9a552f490d320410b28ab5e6c065ee9f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Notice that we use &lt;code&gt;Box::new&lt;/code&gt; to heap-allocate the structure, and &lt;code&gt;Box::into_raw&lt;/code&gt;
to convert it into a raw pointer to return. The corresponding Python code is:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/fcdb3bc11c85147172e1a6ec42f224d0.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;I&amp;rsquo;m not actually sure &lt;code&gt;void*&lt;/code&gt; is the right way to go here, but I don&amp;rsquo;t know any
other way to do opaque pointers in this situation. If you know more about this,
let me know. CFFI seems to understand &lt;code&gt;uint32_t&lt;/code&gt; all on its own, and presumably
will call the Rust function with the appropriate integer width.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe raytracer.py
From Rust: Scene { width: 800, height: 600, fov: 45, elements: [],
    lights: [], shadow_bias: 0.0000000000001, max_recursion_depth: 10 }
From Python: &amp;lt;cdata &#39;void*&#39; 0x000000000155B260&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sharp readers might have noticed that we&amp;rsquo;re leaking Scene objects - we&amp;rsquo;re
allocating some memory on the heap for the boxed Scene and never freeing it.
For this trivial example, it doesn&amp;rsquo;t matter much because it will be cleaned
up when the process terminates, but it is rather inelegant, so let&amp;rsquo;s fix that.&lt;/p&gt;

&lt;h2 id=&#34;disposing-of-allocated-objects&#34;&gt;Disposing Of Allocated Objects&lt;/h2&gt;

&lt;p&gt;This goes back to the brief discussion of ownership earlier. Previously,
Python owned the allocated Vector3 object, so we could trust that it would be
safely freed when it was garbage-collected. Now, we have an object allocated by
Rust, but owned by a pointer in Python. Python doesn&amp;rsquo;t know how to deallocate
an object owned by Rust, so we&amp;rsquo;ll have to return ownership of the pointer to
Rust and allow Rust to free the memory.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2f98fb5590e4dcd895773b1dd39a100d.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Freeing the memory is actually quite simple - we use &lt;code&gt;Box::from_raw&lt;/code&gt; to convert
the raw pointer back into a box, and then just let it fall out of scope. Rust
will automatically clean everything up for us.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/51656ea2be9b8d0c2e7ba2c6ba77bfe4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Right now, there&amp;rsquo;s nothing to stop us from freeing the scene more than once,
or continuing to use that pointer after the scene has been freed. There&amp;rsquo;s nothing
we can do about that from the Rust side, but in Python we can at least build a
safe wrapper to work with.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/ba18f7b5c73c66ad9d1924e80881fc9e.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Here, we define a Python class to represent our Scene. It defines the &lt;code&gt;__enter__&lt;/code&gt;
and &lt;code&gt;__exit__&lt;/code&gt; methods necessary to act as a &lt;a href=&#34;https://www.python.org/dev/peps/pep-0343/&#34;&gt;Context Manager&lt;/a&gt;,
which allows us to use it with the &lt;code&gt;with&lt;/code&gt; statement at the end. Running this
file confirms that the scene object is being freed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe raytracer.py
Freeing the scene
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;enums&#34;&gt;Enums&lt;/h2&gt;

&lt;p&gt;Before we begin constructing our scene in Python, however, there&amp;rsquo;s one more bit
of complexity to tackle first. Every object in this raytracer contains a Material
structure to define what color the surface is, whether it&amp;rsquo;s reflective or
transparent, etc. This is defined in Rust using some enums and a struct:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/5af67a2bc9fb63ad1c77e087d5857c91.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Rust&amp;rsquo;s enums have no equivalent in C, and even if they did that DynamicImage
type certainly doesn&amp;rsquo;t. We&amp;rsquo;ll have to create C-compatible wrappers for these
types that we can expose to Python. I&amp;rsquo;ll focus on the Coloration enum for now,
the SurfaceType enum will work the same way.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start by defining another enum:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/153001fea515624f8eaaf807640c79bb.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;I know, I just said we can&amp;rsquo;t do enums in C. Instead, we&amp;rsquo;ll define a couple of
functions to create CColoration values on the heap and return opaque pointers
to them like we did with the Scene.&lt;/p&gt;

&lt;p&gt;First, the simple case of a solid color:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/37004afc200aa009204031f1443a2ad9.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then, the more complex case of a path to a texture file.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/8fc77676d46de395b661c89bb0f5384d.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Here we take a pointer to a null-terminated character array (a C-style string)
and convert it to a Rust string, which has a length and is encoded in UTF-8.
This conversion could fail, if the C string isn&amp;rsquo;t valid UTF-8. Notice that we
need to be very careful not to panic. We can&amp;rsquo;t just unwrap the result of
converting the CStr to a regular string, because panicking across FFI boundaries
is undefined behavior. Instead, we return a null pointer on all error
conditions. A more serious project would probably want to have more robust error
handling, but this is sufficient for now.&lt;/p&gt;

&lt;p&gt;The corresponding Python should be relatively familiar by now:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/02f53a18cc343e57b3a6e9b0c31678a4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The SurfaceType enum works basically the same way as above, so I&amp;rsquo;ll spare you
the details.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/fbfb35e8b010c89e7b40cc5b93b65a2c.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;All those with&amp;rsquo;s are kind of ugly, but that&amp;rsquo;s the price we pay for safety.
We can verify that everything is being freed as expected:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe raytracer.py
Freeing surface type
Freeing surface type
Freeing surface type
Freeing coloration
Freeing coloration
Freeing coloration
Freeing coloration
Freeing coloration
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;constructing-the-scene&#34;&gt;Constructing the Scene&lt;/h2&gt;

&lt;p&gt;Finally, we&amp;rsquo;re ready to start constructing the scene. I&amp;rsquo;ll focus on the case of
adding a Sphere to the scene. The code to define other objects is pretty much
the same.&lt;/p&gt;

&lt;p&gt;First, we need a new struct to represent Material:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/b90e0a5de2bdbc16d0120142bf6b94ba.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;And a function to add a sphere to a scene:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/bede2aa436be20ede3c7c876f2f28488.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Most of this is the now-familiar C foreign-function boilerplate. The
&lt;code&gt;material.to_rust()&lt;/code&gt; method works pretty much as you&amp;rsquo;d expect - it
constructs a Material value from a CMaterial value, potentially loading the
texture contained in the &lt;code&gt;CColoration&lt;/code&gt;. More noteworthy is the way we convert
the scene Box back into a raw pointer at the end of the method. This prevents
Rust from deallocating our scene.&lt;/p&gt;

&lt;p&gt;You might reasonably ask why I chose to have one function that creates and adds
the sphere directly to the scene. This does, after all, make it impossible for
me to return a Sphere to Python. The answer is that since I don&amp;rsquo;t really want to
manipulate Spheres in Python, there&amp;rsquo;s not much point in going to all that extra
effort. You can go ahead and do that if you like.&lt;/p&gt;

&lt;p&gt;Now that we have all of that, we can call it from Python as before:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/690a6cc77c1e3ba295f5ce45798b6f26.js&#34;&gt;&lt;/script&gt;

&lt;pre&gt;&lt;code&gt;$ python.exe raytracer.py
Sphere { center: Point { x: 0, y: 0, z: -5 }, radius: 1, material:
  Material { coloration: Texture, albedo: 0.18, surface:
  Reflective { reflectivity: 0.7 } } }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;rendering-and-returning-the-image-to-python&#34;&gt;Rendering and Returning the Image To Python&lt;/h2&gt;

&lt;p&gt;Now that we can define a scene in Python, we need a way to render it and return
the resulting image. We can&amp;rsquo;t just return a byte array, because Python can&amp;rsquo;t
handle stack-allocated objects, and anyway it would overflow the stack. We could
return a pointer/length pair, but then we have to pass it back to Rust to free
it. Instead, we&amp;rsquo;ll follow the C convention and have the caller provide a buffer
to render the image into.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/5c3983559358ef40d4de64d83a44cf61.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;After the usual boilerplate, we convert the C-style byte array into a mutable
slice with the &lt;code&gt;slice::from_raw_parts_mut&lt;/code&gt; function, then wrap that into an
ImageBuffer and pass it to the raytracer for rendering. Slices in Rust don&amp;rsquo;t
own their contents, so we don&amp;rsquo;t need to do anything special to prevent Rust from
trying to free the buffer.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/51eb8b1f5ad6067942d8a1a969842185.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;In Python, we need to save the dimensions of the image so that we can allocate
an appropriate buffer. The raytracer uses 4-byte RGBA pixels, so we calculate
the buffer size as 4 * width * height, allocate an appropriate buffer, and
render the image into it. Then we call &lt;code&gt;ffi.buffer&lt;/code&gt; to wrap it into a convenient
Python object. Finally, we pass that to the Pillow library to be wrapped into
an Image object that we can save out to disk or do further processing on.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://imgur.com/a/knyif&#34;&gt;&lt;img src=&#34;https://bheisler.github.io/static/rendered-by-python.png&#34; alt=&#34;Rendered By Python&#34; /&gt;&lt;/a&gt;
Click to see high-resolution image&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Overall, this turned out to be easier than I&amp;rsquo;d expected. CFFI&amp;rsquo;s user-friendly
interface helped a lot, I think, though the Rust side has a lot of boilerplate.
I expect some macros or something could help with that. I&amp;rsquo;d like to thank Jake
Goulding and co. for the &lt;a href=&#34;http://jakegoulding.com/rust-ffi-omnibus/slice_arguments/&#34;&gt;Rust FFI Omnibus&lt;/a&gt;,
which covers all of the basic techniques listed above (and provides examples
for a number of other languages, if you&amp;rsquo;d like to compare).&lt;/p&gt;

&lt;p&gt;As usual, if you want to try playing around with the code yourself, you can
check out the &lt;a href=&#34;https://github.com/bheisler/raytracer&#34;&gt;GitHub Repository&lt;/a&gt;. If you
do, though, be careful with the complexity of the scene you try to render. It&amp;rsquo;s
very easy to reach multi-hour rendering times when you&amp;rsquo;re defining scenes
programmatically. Otherwise, enjoy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing a Raytracer in Rust - Part 3 - Reflection and Refraction</title>
      <link>https://bheisler.github.io/post/writing-raytracer-in-rust-part-3/</link>
      <pubDate>Mon, 27 Mar 2017 00:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/writing-raytracer-in-rust-part-3/</guid>
      <description>

&lt;p&gt;Hello again, and welcome to the final part of my series on writing a raytracer
in Rust (&lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/&#34;&gt;Part 1&lt;/a&gt;,
&lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-2/&#34;&gt;Part 2&lt;/a&gt;). Previously we implemented
a basic raytracer which could handle diffuse shading of planes and spheres with
multiple objects and multiple lights. This time, we&amp;rsquo;ll add texturing, reflection
and transparent objects.&lt;/p&gt;

&lt;p&gt;First, I&amp;rsquo;ve refactored the common parts of Sphere and Plane out to a separate
structure. Since this post is all about handling more complex surface properties,
we&amp;rsquo;ll need a structure to represent them and avoid duplication.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/f49634600ec7910c149e577804a5e3cc.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;texturing&#34;&gt;Texturing&lt;/h2&gt;

&lt;p&gt;In order to texture our objects, we need to do two things. First, we need to
calculate the texture coordinates corresponding to the point on the object that
the ray intersected. Then we need to look up the color at those coordinates.
We&amp;rsquo;ll start by introducing a structure to contain our texture coordinates and a
function to calculate them.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/952f43be37463e22d7db93800f12ee21.js&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;spheres&#34;&gt;Spheres&lt;/h3&gt;

&lt;p&gt;The texture coordinates of a sphere are simply the spherical coordinates of the
intersection point. If our sphere were the Earth, these would be akin to the
latitude and longitude.&lt;/p&gt;

&lt;p&gt;We can compute the (x, y, z) coordinates of the intersection relative to the
center of the sphere by using the vector subtraction of the hit point and the
center of the sphere. Then, we can convert those to the spherical coordinates
using these formulas:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;phi = atan(z, x)
theta = acos(y/R) //Where R is the radius of the sphere
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If your trigonometry is rusty, check out &lt;a href=&#34;https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/spherical-coordinates-and-trigonometric-functions&#34;&gt;this explanation&lt;/a&gt;
at (you guessed it)
Scratchapixel. If you do though, be aware that their coordinates have the Z and
Y axes swapped so they have slightly different formulas.&lt;/p&gt;

&lt;p&gt;These formulas produce values in the range (-pi&amp;hellip;pi) and (0&amp;hellip;pi) respectively.
We want (0&amp;hellip;1) instead so we&amp;rsquo;ll adjust the formula to remap the values to the
correct range:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tex.x = (1 + atan(z, x) / pi) * 0.5
tex.y = acos(y) / pi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have something we can implement in code, like so:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/fe8a5ba0f4ee0f7d835b100126220636.js&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;planes&#34;&gt;Planes&lt;/h3&gt;

&lt;p&gt;To calculate the texture coordinates on a plane, we first need to construct a
coordinate system (that is, two perpendicular unit vectors) aligned with that
plane. This can be done using the cross product, which takes two vectors and
produces a new vector which is perpendicular to them. In this case, we use the
surface normal and the forward vector (unless the normal is equal to the forward
vector, in which case use the up vector). This produces one vector parallel to
the plane to be our X axis. To get the other vector, we can just cross the
normal with the X axis.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/0c2213af556812edd3bae4f9827a666c.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then we can compute the texture coordinates by taking the dot product of the
vector from the hit location to the origin against the axes (effectively
separating the hit vector into its X and Y components).&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/c6f60e809599459aa216bfb0391b3adf.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;It might be useful to add a scaling factor and an offset to allow the user to
adjust the position and size of the texture, but this is left as an exercise for
the reader.&lt;/p&gt;

&lt;p&gt;Next we need to add the texture to our scene.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/655602ee3ed04f35a98a2e4d7a5240a4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then we can look up the color based on the texture coordinates.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/f7aa43da0d4a535085b3af2b01538224.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/textured-objects.png&#34; alt=&#34;Textured Objects&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;reflection&#34;&gt;Reflection&lt;/h2&gt;

&lt;p&gt;Conceptually, implementing reflection in a raytracer is quite simple. When a ray
hits a reflective object, just trace another ray out from the intersection at
the appropriate angle - recursively if necessary - and mix the color value from
that ray in with the color value of the first ray.&lt;/p&gt;

&lt;p&gt;As usual, the first thing to do is extend the scene definition. Since the
reflection process is recursive, we also add a value for the maximum recursion
depth. Deeper recursion will produce a more accurate image, but at the cost of
increased rendering time.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/0ff69bf512f8f62ce59d09ec79f222ad.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The reflectivity controls how much of the final pixel color comes from the
reflection and how much from the object itself. If the reflectivity is zero,
we&amp;rsquo;ll use the diffuse color and there will be no reflection. If the reflectivity
is one, we&amp;rsquo;ll use the reflected color and the object will appear to be a
perfect mirror. If the value is somewhere in between, we could get effects
ranging from &amp;lsquo;glossy surface&amp;rsquo; to &amp;lsquo;tinted chrome.&amp;rsquo;&lt;/p&gt;

&lt;p&gt;Since the last part, I&amp;rsquo;ve extracted most of what was in get_color into a function
for doing diffuse shading so that we can use get_color for mixing together the
reflection and diffuse colors.&lt;/p&gt;

&lt;p&gt;As you can see, we construct a reflection ray and trace it through the scene
like with our prime ray, then mix it in with the diffuse color. We also track
the current recursion depth and simply return black if we reach the limit.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/ed31642dcd5a3b58ea1912348e5ae70c.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The more interesting question here is how to compute the reflection ray. If
you&amp;rsquo;ve taken physics, you may remember the mantra that the angle of incidence
equals the angle of reflection. That&amp;rsquo;s helpful enough as far as it goes, but
how do we actually calculate that in terms of vectors?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/reflection-ray.png&#34; alt=&#34;Reflection Ray&#34; align=&#34;right&#34;&gt;&lt;/p&gt;

&lt;p&gt;We can separate the incident vector I into two vectors, A and B (see figure)
such that I = A + B. The reflection vector R is then equal to A - B.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I = A + B
R = A - B
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can compute B quite easily - it&amp;rsquo;s the projection of I onto the surface normal,
or the dot product of I and N multiplied by N.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;B = (I.N)N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Substitute that into both equations:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I = A + (I.N)N
R = A - (I.N)N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then solve the first equation for A:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A = I - (I.N)N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And substitute into the second equation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;R = I - (I.N)N - (I.N)N
R = I - 2(I.N)N
&lt;/code&gt;&lt;/pre&gt;

&lt;script src=&#34;//gist.github.com/bheisler/3a29db0b3104d4420e80523268f3607a.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;We also adjust the origin slightly along the surface normal to avoid the same
floating-point precision problems we had with our shadows earlier.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://imgur.com/a/Kuwks&#34;&gt;&lt;img src=&#34;https://bheisler.github.io/static/reflective-objects.png&#34; alt=&#34;Reflective Objects&#34; /&gt;&lt;/a&gt;
Click to see high-resolution image. Note the recursive reflections between the
center sphere and the floor.&lt;/p&gt;

&lt;h2 id=&#34;refraction&#34;&gt;Refraction&lt;/h2&gt;

&lt;p&gt;Refraction is again conceptually simple in a raytracer - trace a secondary ray
(called the transmission ray) through the object in the appropriate direction
and mix it in with the color of the object. Unfortunately, the math to construct
the transmission ray is a lot more complex than it is to construct the
reflection ray. But first, some definitions:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/04fc2c624e4c3cfae1171a2660a16c2a.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The transparency is the same as the reflectivity from before - the fraction of
the final color that comes from refraction. Refraction is governed by a parameter
called the index of refraction. When a ray of light passes from one transparent
substance to another, it bends at an angle described by Snell&amp;rsquo;s Law:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/Snells_law2.svg&#34; alt=&#34;Snell&#39;s Law&#34; align=&#34;right&#34;&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sin(theta_i)/sin(theta_t) = eta_t/eta_i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where theta_i and theta_t are the angle of incidence and angle of transmission,
and eta_i and eta_t are the indices of refraction for the incident substance and
the transmitting substance. We could calculate the angle of transmission using
this equation, but we&amp;rsquo;ll need to do more to convert that angle into a vector.&lt;/p&gt;

&lt;p&gt;As with reflection, refraction is really a two-dimensional process in the plane
formed by the incident vector and the surface normal. This means that we can
think of our transmitted ray as having a horizontal component (A) and vertical
component (B). B is relatively simple to calculate:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;B = cos(theta_t) * -N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This makes some intuitive sense - the transmitted ray will be on the opposite
side of the surface from the incident ray, so it&amp;rsquo;s vertical component will be
some fraction of the inverse of the surface normal. We use the cosine of the
transmission angle because that&amp;rsquo;s how you calculate the vertical distance.&lt;/p&gt;

&lt;p&gt;We can use this same approach to get the horizontal component A, but first we
need to construct a horizontal unit vector (M). To do this, we first take the
incident vector and subtract it&amp;rsquo;s vertical component, leaving only the
horizontal component. We can calculate the vertical component of I easily -
it&amp;rsquo;s (I.N)N, just like before. Then we normalize this horizontal vector to get
the horizontal unit vector we need. We can slightly cheat here, though - the
length of the horizontal component of I will be equal to sin(theta_i), so we
can normalize using that instead of computing the vector length the slow way.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;M = (I - -N(I.N)) / sin(theta_i) = (I + N(I.N)) / sin(theta_i)
A = sin(theta_t) * M
B = cos(theta_t) * -N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all back together, we get:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;T = A + B
T = (sin(theta_t) * M) - N * cos(theta_t)
T = (sin(theta_t) * (I + N(I.N)) / sin(theta_i)) - N * cos(theta_t)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use Snell&amp;rsquo;s Law to replace that sin(theta_t) / sin(theta_i) with
eta_i/eta_t, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;T = (I + N(I.N)) * eta_i/eta_t - N * cos(theta_t)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We could calculate cos(theta_t) from Snell&amp;rsquo;s Law and theta_i, but this involves
lots of trigonometry, and ain&amp;rsquo;t nobody got time for that. Instead, we can
express that in terms of a dot-product. We know from trigonometry that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cos^2(theta_t) + sin^2(theta_t) = 1
cos(theta_t) = sqrt(1 - sin^2(theta_t))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And from Snell&amp;rsquo;s Law we know that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sin(theta_t) = (eta_i/eta_t) * sin(theta_i)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Therefore:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cos(theta_t) = sqrt( 1 - (eta_i/eta_t)^2 * sin^2(theta_1) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can use the same trigonometric identity from above to convert that sin
to a cosine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cos(theta_t) = sqrt( 1 - (eta_i/eta_t)^2 * (1 - cos^2(theta_i)) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And since cos(theta_i) = I.N, we get:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cos(theta_t) = sqrt( 1 - (eta_i/eta_t)^2 * (1 - I.N^2) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And so, finally, we arrive at this monster of an equation (but look, no trigonometry):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;T = (I + N(I.N)) * eta_i/eta_t - N * sqrt( 1 - (eta_i/eta_t)^2 * (1 - I.N^2) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, there are a couple of wrinkles left to sort out. First, sometimes our ray
will be leaving the transparent object rather than entering it. This is easy
enough to handle, just invert the normal and swap the indices of refraction. We
also need to handle total internal reflection. In some cases, if the angle of
incidence is shallow enough, the refracted light ray actually reflects off the
surface instead of passing through and travels back into the object. We can
detect this when the term inside the sqrt is negative. Again, this makes
intuitive sense - if that&amp;rsquo;s negative, the vertical component of the transmission
vector would be positive (remember, B is a multiple of -N) and therefore on the
same side of the surface as the incident vector. In fact, however, we can handle
this by completely ignoring it, and I&amp;rsquo;ll explain why later.&lt;/p&gt;

&lt;p&gt;Whew! Now that we have that giant equation, we can implement it in code, like so:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/ceba1d949b9b766a03d275534f016fd3.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;I also discovered a nasty bug in my sphere-intersection code while testing this.
If you&amp;rsquo;re following along at home, this could be a good opportunity to practice
your debugging. Go ahead, I&amp;rsquo;ll wait.&lt;/p&gt;

&lt;p&gt;Hint: What happens if the ray origin is inside the sphere?&lt;/p&gt;

&lt;p&gt;Find it? It turns out that the sphere-intersection test will return a point
behind the origin of the ray if the origin is inside the sphere. The refraction
ray will then intersect the sphere again, creating another refraction ray and
so on until we hit the recursion limit. This took hours of painful debugging to
find because I was looking for bugs in the create_transmission function. I
didn&amp;rsquo;t realize that it was something else until I tried to create a refractive
plane and noticed that it appeared to behave correctly.&lt;/p&gt;

&lt;p&gt;Anyway, here&amp;rsquo;s the corrected sphere-intersection function:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/393169985793f91cdd71b5616faace99.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;http://imgur.com/a/T9F6O&#34;&gt;&lt;img src=&#34;https://bheisler.github.io/static/initial-refraction.png&#34; alt=&#34;Initial Refraction&#34; /&gt;&lt;/a&gt;
Click to see high-resolution image. Notice the refracted image of the floor in
the transparent sphere.&lt;/p&gt;

&lt;h2 id=&#34;fresnel&#34;&gt;Fresnel&lt;/h2&gt;

&lt;p&gt;However, we&amp;rsquo;re not quite done yet. If you&amp;rsquo;ve ever noticed how glass buildings or
smooth lakes look like mirrors far away but clear up close, you know that
transparent surfaces reflect light as well as transmitting it. It&amp;rsquo;s often even
possible to see this effect in the polished floors of long hallways. These
reflections are governed by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fresnel_equations&#34;&gt;Fresnel Equations&lt;/a&gt;
and we&amp;rsquo;ll have to simulate them to render refractive objects more accurately.
Incidentally, this is why we can ignore total internal reflection
in our transmission code above - the Fresnel code will cover that for us.
We already know how to handle reflection in our code, but we need to calculate
how much of a ray&amp;rsquo;s color comes from the refraction and how much from the
reflection.&lt;/p&gt;

&lt;p&gt;The derivation of the Fresnel Equations is hairy enough that Scratchapixel
doesn&amp;rsquo;t even try to explain it. Serious physics-lovers might want to check out
&lt;a href=&#34;http://physics.gmu.edu/~ellswort/p263/feqn.pdf&#34;&gt;this derivation&lt;/a&gt; (PDF), but
this is getting out of my depth so I&amp;rsquo;ll just take the final equations as given.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/fresnel-equations.png&#34; alt=&#34;Fresnel Equations&#34; align=&#34;center&#34;&gt;&lt;/p&gt;

&lt;p&gt;Fortunately for me, Scratchapixel does include some C++ code implementing these
equations that I can simply translate to Rust:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/767895b07240a346bcf4353826d269f3.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;And now that we have that, we can put it all together:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2d072e000863e30c7337d3588ff81291.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;http://imgur.com/EvmGhQW&#34;&gt;&lt;img src=&#34;https://bheisler.github.io/static/complete-refraction.png&#34; alt=&#34;Complete Refraction&#34; /&gt;&lt;/a&gt;
Click to see high-resolution image&lt;/p&gt;

&lt;p&gt;Beautiful, isn&amp;rsquo;t it?&lt;/p&gt;

&lt;h2 id=&#34;addendum-gamma-correction&#34;&gt;Addendum - Gamma Correction&lt;/h2&gt;

&lt;p&gt;After I posted this article, &lt;a href=&#34;https://github.com/fstirlitz&#34;&gt;fstirlitz&lt;/a&gt;
&lt;a href=&#34;https://github.com/bheisler/raytracer/issues/2&#34;&gt;pointed out&lt;/a&gt; that the lighting
was a bit off because my code wasn&amp;rsquo;t handling Gamma Correction correctly (or at
all). I had noticed the strange lighting effects but had simply assumed they
were an artifact of my relatively simple rendering process, and that a more
advanced renderer would solve them.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll spare you a detailed discussion of what Gamma Correction is (if you&amp;rsquo;re
interested, see &lt;a href=&#34;http://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/&#34;&gt;What Every Coder Should Know About Gamma&lt;/a&gt; by John Novak).
The short version is that human perception of light is non-linear. Our screens
account for that by applying a power-law formula to the pixel values before
displaying them. Correspondingly, image formats expect the pixel values to be
in this non-linear color space, where my raytracer was writing out pixels in a
linear color space. Likewise, when reading textures, it was reading non-linear
color values and treating them as if they were linear. This mismatch caused
the image to be generally darker, with sharper divisions between light and dark
shades than the human eye would have seen.&lt;/p&gt;

&lt;p&gt;Fortunately, this is pretty easy to fix:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/972da2540646d4195b23a76db9fdd3ac.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;I&amp;rsquo;ve only implemented the basic power-law formula rather than the more complex
sRGB conversion, but it&amp;rsquo;s good enough for now.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://imgur.com/a/tNhlk&#34;&gt;&lt;img src=&#34;https://bheisler.github.io/static/gamma-corrected.png&#34; alt=&#34;Gamma Corrected&#34; /&gt;&lt;/a&gt;
Click to see high-resolution image&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is the end of my series on raytracing, at least for now. There are many,
many things I didn&amp;rsquo;t even begin to cover here. For instance, you might notice
how the two lights in this scene don&amp;rsquo;t glint off the reflective objects the
way real lights would, and how the glass sphere on the right doesn&amp;rsquo;t focus
light rays on the floor like real glass would. If you&amp;rsquo;re interested, I encourage
you to dig deeper - I may return to this subject myself in the future. Until
then, I hope you&amp;rsquo;ve enjoyed reading.&lt;/p&gt;

&lt;p&gt;As before, if you want to try playing around with the code yourself, you can
check out the &lt;a href=&#34;https://github.com/bheisler/raytracer&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing a Raytracer in Rust - Part 2 - Light and Shadow</title>
      <link>https://bheisler.github.io/post/writing-raytracer-in-rust-part-2/</link>
      <pubDate>Mon, 20 Mar 2017 00:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/writing-raytracer-in-rust-part-2/</guid>
      <description>

&lt;p&gt;Welcome to Part 2 of my series on writing a raytracer in Rust. If you haven&amp;rsquo;t
already, you may wish to read &lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/&#34;&gt;Part 1&lt;/a&gt;.
Previously, we implemented a basic raytracer which can render only a single
sphere with no lighting. This time, we&amp;rsquo;ll add multiple objects, planes, and
basic lighting.&lt;/p&gt;

&lt;h2 id=&#34;multiple-objects&#34;&gt;Multiple Objects&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s pretty easy to change our scene definition to contain a Vec of spheres
instead of just a single one. Once we have multiple spheres, however, we need to
know which one our ray hit. This is easy if they don&amp;rsquo;t overlap on the screen.
If they do, we can find the correct sphere by taking the nearest intersection
to our camera. That means we need to know the distance to the intersection, not
just whether there is an intersection or not.&lt;/p&gt;

&lt;p&gt;This requires a bit more geometry. Recall from last time that we detect an
intersection by constructing a right triangle between the camera origin and the
center of the sphere. We can calculate the distance between the center of the
sphere and the camera, and the distance between the camera and the right angle
of our triangle. From there, we can use Pythagoras&amp;rsquo; Theorem to calculate the
length of the opposite side of the triangle. If the length is greater than the
radius of the sphere, there is no intersection.&lt;/p&gt;

&lt;p&gt;There are more right triangles formed by the ray than just this one, however.
If we instead create a triangle between the point that the ray intersects the
sphere and the center of the sphere, we can again use Pythagoras&amp;rsquo; Theorem to
calculate the distance from our right angle to the intersection point.
Subtracting that from the distance from the camera to the right angle gives the
distance to the intersection point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/intersection-distance.png&#34; alt=&#34;Intersection Distance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To put that in code, here are the changes to our Sphere intersection method:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/27f0fc17de209662a04cd6393a8731c3.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Now that we know the distance to the intersection, we need a method to perform
the iteration and return the nearest intersection. It&amp;rsquo;s also useful to return
a reference to the object itself (for example, so we can use the right color).
Notice that we have to use partial_cmp and unwrap to compare the distances. This
is an instance where Rust&amp;rsquo;s strict type safety sort of gets in the way - because
some values (NaN, +-Infinity) can&amp;rsquo;t be correctly compared, f64 doesn&amp;rsquo;t implement
the Cmp trait. In this case, no valid intersection can ever contain those values
so we should be safe just using unwrap. It&amp;rsquo;s a bit ugly, but
it&amp;rsquo;s probably better than tracking down strange bugs related to NaN-safety later.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/95ffb04905984a10fe00580851e19380.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;planes&#34;&gt;Planes&lt;/h2&gt;

&lt;p&gt;Next up we&amp;rsquo;ll add Planes as an object to test our rays against. There are a few
ways to represent planes in 3D space, but for our purposes the most convenient
is to define a point on the plane, and the normal of the surface. Before we
implement the intersection test though, we need to adapt our scene structure
so it can contain an arbitrary number of spheres or planes.&lt;/p&gt;

&lt;p&gt;We could try adding another Vec of Plane structures, but that gets annoying
quickly. We&amp;rsquo;d have to duplicate some logic (eg. the trace method) to apply to
both the spheres and the planes. Some sort of dynamic dispatch is appropriate
here. Rust provides two ways to do this. We could either wrap each object in a
variant of an Enum or we could use
&lt;a href=&#34;https://doc.rust-lang.org/book/trait-objects.html&#34;&gt;Trait Objects&lt;/a&gt;. I&amp;rsquo;ve chosen
to go with the former, but it&amp;rsquo;s mostly a matter of personal preference.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/7c4f1d7580a79c636d8b1ddfaecd652f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Now that we have our Plane structure, how can we test for an intersection?
One convenient property of planes is that they&amp;rsquo;re infinitely large. If a plane
isn&amp;rsquo;t perfectly parallel to our ray, it will always intersect eventually.
We can test this with the dot product - if the dot product between the ray and
the normal of the plane is zero (give or take a bit to account for
floating-point error) then it&amp;rsquo;s parallel and thus there is no intersection.
Otherwise, there is an intersection somewhere.&lt;/p&gt;

&lt;p&gt;However, we need to know where that intersection is. I&amp;rsquo;m afraid that I haven&amp;rsquo;t
been able to find a good intuitive or geometric explanation of why this works,
so I&amp;rsquo;ll just have to direct you to
&lt;a href=&#34;https://www.scratchapixel.com/lessons/3d-basic-rendering/minimal-ray-tracer-rendering-simple-shapes/ray-plane-and-ray-disk-intersection&#34;&gt;Scratchapixel&lt;/a&gt;,
where they show the full derivation of the equation.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/388e88f4cb2ecb391c63aabd76859c5f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Now that all of that&amp;rsquo;s done, let&amp;rsquo;s take a moment to admire our handywork.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/spheres-and-planes.png&#34; alt=&#34;Spheres and Planes&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Yeah, I know. Five minutes in MS Paint, amirite? It will look better once we
start adding lighting effects.&lt;/p&gt;

&lt;h2 id=&#34;directional-lights&#34;&gt;Directional Lights&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll start by adding a single directional light to our scene. Directional lights
approximate light from the sun or stars - objects so far away that their light
rays are effectively parallel to each other and at an approximately-constant
intensity level. As a result, they&amp;rsquo;re also simpler than closer point-source
lights.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/5884657ad43bb968c8dbcedddfe72faf.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Next we need to know the surface normal of the object at the point our ray
intersected with it.&lt;/p&gt;

&lt;p&gt;Sphere:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fn surface_normal(&amp;amp;self, hit_point: &amp;amp;Point) -&amp;gt; Vector3 {
    (*hit_point - self.center).normalize()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plane:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fn surface_normal(&amp;amp;self, _: &amp;amp;Point) -&amp;gt; Vector3 {
    -self.normal
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we&amp;rsquo;ll need to add an albedo to our Spheres and Planes. This is simply
a parameter which specifies how much light energy is reflected by an object and
how much is absorbed.&lt;/p&gt;

&lt;p&gt;Now to actually implement the shading. First some preparation.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let intersection = scene.trace(&amp;amp;ray);
let hit_point = ray.origin + (ray.direction * intersection.distance)
let surface_normal = intersection.element.surface_normal(&amp;amp;hit_point)
let direction_to_light = -scene.light.direction
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we calculate the amount of light that lands on this point. This is
proportional to the cosine of the angle between the surface normal and the
direction to the light (&lt;a href=&#34;https://en.wikipedia.org/wiki/Lambert%27s_cosine_law&#34;&gt;Lambert&amp;rsquo;s Cosine Law&lt;/a&gt;).
The dot product is the length of one vector times the
cosine of the angle between them, but because we use normalized vectors the
length will be one. We also add a factor for the brightness of the light.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let light_power = (surface_normal.dot(&amp;amp;direction_to_light) as f32) *
    scene.light.intensity;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we calculate the proportion of the light which is reflected. This is equal
to the albedo of the object divided by Pi. Once again I have to admit that I
can&amp;rsquo;t find a good explanation of this formula. If you&amp;rsquo;re really interested, you
can once again check out Scratchapixel&amp;rsquo;s
&lt;a href=&#34;https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/diffuse-lambertian-shading&#34;&gt;derivation&lt;/a&gt;
(be warned - this one contains integrals). The short version is that dividing by
Pi ensures that the object doesn&amp;rsquo;t reflect away more energy than it receives.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let light_reflected = intersection.element.albedo() / std::f32::consts::PI;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we accumulate this together into the final color for the pixel.
We represent colors as (R, G, B) triplets where each value is in the range
0.0&amp;hellip;1.0. We can multiply colors by multiplying each value - eg. if the red
component of a light is 0.5 and the object reflects 0.5 of red light, the viewer
will receive a red value of 0.25.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let color = intersection.element.color() * scene.light.color *
            light_power * light_reflected;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or, all together:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/7fe4960b607344aa57a06d4712685ab5.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/directional-lighting.png&#34; alt=&#34;Directional Lighting&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s still not quite right though - none of the spheres are casting shadows, on
the lower plane or on each other.&lt;/p&gt;

&lt;h2 id=&#34;shadows&#34;&gt;Shadows&lt;/h2&gt;

&lt;p&gt;Calculating shadows in a raytracer is really easy. Simply trace another ray from
the intersection of the prime ray and the object back to the light. If there is
another object between the intersection and the light, the point is in shadow.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/14fae787f7092ad068a572d9b406d10f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/shadow-acne.png&#34; alt=&#34;Shadow Acne&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Well, that&amp;rsquo;s not quite right. We have shadows on the lower plane and the green
sphere, but also a lot of noise. The dark noise is called &amp;lsquo;shadow acne&amp;rsquo; and
it occurs because our floating point values have limited precision. Sometimes,
the hit point will be ever so slightly inside the intersected object and so the
shadow ray will intersect with the same object the prime ray did. It might seem
like we could simply ignore that object when tracing the shadow ray, and for
this simple geometry we could. If we had more complex objects though (eg. a model of
a tree) we would want an object to be able to cast shadows on itself, so that
won&amp;rsquo;t work. Instead, we simply add a tiny fudge factor and adjust the origin
of the shadow ray a short distance along the surface normal so that we can be
sure it&amp;rsquo;s outside the object. It doesn&amp;rsquo;t have to be much - I&amp;rsquo;ve found that bias
values as small as 1e-13 were enough to eliminate visible shadow acne.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let shadow_ray = Ray {
    origin: hit_point + (surface_normal * scene.shadow_bias),
    direction: direction_to_light,
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/shadows.png&#34; alt=&#34;Shadows&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;multiple-lights&#34;&gt;Multiple Lights&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s pretty easy to implement multiple lights as well. The light the camera sees
from any particular point is equal to the sum of the contributions from each
individual light source. We can just iterate through the lights, accumulating
together the color values from each.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/c6186ef183ed98fccd02c119c2cc01a4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;This produces the following image - notice the two sets of shadows.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/multiple-lights.png&#34; alt=&#34;Multiple Lights&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;spherical-lights&#34;&gt;Spherical Lights&lt;/h2&gt;

&lt;p&gt;Finally, we&amp;rsquo;ll add Spherical Lights (or point lights). First some definitions.
Again, I&amp;rsquo;m using an enum for dynamic dispatch.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2aad485bfef7087438c493e3ca6a5bdc.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Next up, we need to know the direction to the light. This is easily calculated:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(s.position - *hit_point).normalize()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The intensity of these lights obeys the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Inverse-square_law&#34;&gt;Inverse Square Law&lt;/a&gt;, so we
calculate the intensity by dividing the light&amp;rsquo;s intensity value by 4*Pi*distance^2.
Incidentally, this means that the intensity values of spherical lights in your
scene definition must be much larger than for directional lights.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let r2 = (s.position - *hit_point).norm() as f32;
s.intensity / (4.0 * ::std::f32::consts::PI * r2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Additionally, our shadow test needs to be changed a bit. For directional lights,
we only had to check if there was any intersection in the direction of the light.
That won&amp;rsquo;t work now - what if there&amp;rsquo;s an object on the far side of the light?
Instead we check if the nearest intersection is closer than the light itself is.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let shadow_intersection = scene.trace(&amp;amp;shadow_ray);
let in_light = shadow_intersection.is_none() ||
    shadow_intersection.unwrap().distance &amp;gt; light.distance(&amp;amp;hit_point);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting that all together produces this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/spherical-lights.png&#34; alt=&#34;Spherical Lights&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Try doing that in five minutes in MS Paint!&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve taken this toy raytracer from producing an image of a green circle to a
nicely-lit scene containing multiple objects. The &lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-3/&#34;&gt;last entry&lt;/a&gt;
in this series will go on to add texturing as well as simple reflection and
refraction simulations. As before, if you want to try playing around with the
code yourself, you can check out the
&lt;a href=&#34;https://github.com/bheisler/raytracer&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing a Raytracer in Rust - Part 1 - First Rays</title>
      <link>https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/</link>
      <pubDate>Mon, 20 Feb 2017 11:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/</guid>
      <description>

&lt;p&gt;Hello! This is part one of a short series of posts on writing a simple raytracer
in Rust. I&amp;rsquo;ve never written one of these before, so it should be a learning
experience all around.&lt;/p&gt;

&lt;p&gt;So what is a raytracer anyway? The short version is it&amp;rsquo;s a computer program that
traces the paths of simulated rays of light through a scene to produce
high-quality 3D-rendered images. Despite that, it also happens to be the simplest
way to render 3D images. Unfortunately, that comes at a cost in render time -
raytracing an image takes much longer than the polygon-based rendering done by
most game engines. This means that raytracing is typically used to produce
&lt;a href=&#34;http://hof.povray.org/&#34;&gt;beautiful still images&lt;/a&gt; or pre-rendered video (eg.
Pixar&amp;rsquo;s &lt;a href=&#34;https://renderman.pixar.com/&#34;&gt;RenderMan&lt;/a&gt; technology).&lt;/p&gt;

&lt;p&gt;For the purposes of this post, I&amp;rsquo;ll assume that you&amp;rsquo;re familiar with what
vectors are and how they work, as well as basic geometry. If you aren&amp;rsquo;t, check
out the first three pages of Scratchapixel&amp;rsquo;s
&lt;a href=&#34;https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/points-vectors-and-normals&#34;&gt;excellent series&lt;/a&gt;
on geometry and linear algebra. You don&amp;rsquo;t need to know Rust specifically (though
I recommend it, it&amp;rsquo;s a great language) but you should at least be familiar with
C-family programming languages. If you want to build the code however, you will
need to install &lt;a href=&#34;https://rustup.rs/&#34;&gt;Cargo&lt;/a&gt;, which is the standard Rust build
tool.&lt;/p&gt;

&lt;h2 id=&#34;defining-the-scene&#34;&gt;Defining the Scene&lt;/h2&gt;

&lt;p&gt;The first thing to do is decide exactly what our scene (and therefore our
renderer) will be able to handle. For this first post, it won&amp;rsquo;t be much. One
lonely sphere, hanging in the darkness. No lighting, reflection, or transparency.
No other shapes. We&amp;rsquo;ll extend this basic scene over the rest of this series.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start by defining some structures to hold our scene data:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/bf4247cf7921d8c449e3cd62f323519d.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;And a stub render method and simple test case. I&amp;rsquo;m using the
&lt;a href=&#34;https://crates.io/crates/image&#34;&gt;image crate&lt;/a&gt; to set up the image buffer and
write the resulting render to a PNG file. This is all pretty straightforward
except for the position of the sphere - (0.0, 0.0, -5.0). I&amp;rsquo;ll explain that
later.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/1eb2e5fadc9edb680760360ee53f9a78.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;prime-ray-generation&#34;&gt;Prime Ray Generation&lt;/h2&gt;

&lt;p&gt;The basic idea of how a raytracer like this works is that we iterate over every
pixel in the finished image, then trace a ray from the camera out through that
pixel to see what it hits. This is the exact opposite of how real light works,
but it amounts to pretty much the same thing in the end. Rays traced from the
camera are known as prime rays or camera rays. There is actually a lot of
freedom in how we translate pixel coordinates to prime rays, which confused me
for a while, but it&amp;rsquo;s pretty simple if you follow common conventions.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start with defining a Ray structure and a static function for generating
prime rays:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/e8f47c4cad5b1210231d66200846f653.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;By convention, the camera is aligned along the negative z-axis, with positive x
towards the right and positive y being up. That&amp;rsquo;s why the sphere is at
(0.0, 0.0, -5.0) - it&amp;rsquo;s directly centered, five units away from the camera.
We&amp;rsquo;ll start by pretending there&amp;rsquo;s a two-unit by two-unit square one unit in
front of the camera. This square represents the image sensor or film of our camera.
Then we&amp;rsquo;ll divide that sensor square into pixels, and use the directions to each
pixel as our rays. We need to translate the (0&amp;hellip;800, 0&amp;hellip;600) coordinates of our
pixels to the (-1.0&amp;hellip;1.0, -1.0&amp;hellip;1.0) coordinates of the sensor. I&amp;rsquo;ll start
with the finished code for this step, then explain it in more detail.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/be79c6e0871e4308443c0d4e61318fed.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Let&amp;rsquo;s unpack that a bit and focus on only the x component. The y component is
almost exactly the same.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let pixel_center = x as f64 + 0.5;
let normalized_to_width = pixel_center / screen.width as f64;
let adjusted_screen_pos = (normalized_to_width * 2.0) - 1.0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, we cast to float and add 0.5 (one half-pixel) because we want our ray to
pass through the center (rather than the corner) of the pixel on our imaginary
sensor. Then we divide by the image width to convert from our original
coordinates (0&amp;hellip;800) to (0.0&amp;hellip;1.0). That&amp;rsquo;s almost, but not quite, the
(-1.0&amp;hellip;1.0) coordinates we want, so we multiply by two and subtract one. That&amp;rsquo;s
all there is to it! The y calculation follows the same basic process except the
last step:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let adjusted_screen_pos = 1.0 - (normalized_to_width * 2.0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is simply because the image coordinates have positive y meaning down, where
we want positive y to be up. To correct for this, we simply take the negative of
the last step of the calculation.&lt;/p&gt;

&lt;p&gt;Then we pack the x and y components into a vector (z is -1.0 because all
of our prime rays should go forward from the camera) and normalize it to get a
nice direction vector. Simple, right? This is why the 2x2 sensor 1 unit from the
camera convention is convenient. If we&amp;rsquo;d used any other set of coordinates than
(-1.0&amp;hellip;1.0, -1.0&amp;hellip;1.0) then the image would be off center and/or we&amp;rsquo;d have to
do more calculations to avoid distorting it.&lt;/p&gt;

&lt;p&gt;We could actually stop here - this is a working prime ray generation function.
However, it assumes that the image we&amp;rsquo;re generating is perfectly square and that
the field of view is precisely 90 degrees. It&amp;rsquo;s probably worth adding a
correction for other aspect ratios and different fields of view.&lt;/p&gt;

&lt;p&gt;To adjust for different aspect ratios, we calculate the aspect ratio and
multiply it by the x coordinate. We&amp;rsquo;re assuming that the image will be wider than
it is tall, but most images are so that&amp;rsquo;s good enough for now. If we didn&amp;rsquo;t do
this, the rays would be closer together in the x direction than in the y, which
would cause a distortion in the image (where every pixel is the same size in
both directions).&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/1bef7641a1ce2e52957f65a9022e6a0f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then we can add another adjustment for field of view. Field of view is the angle
between the left-most ray and the right-most ray (or top- and bottom-most). We
can use simple trigonometry to calculate how much we need to adjust the
coordinates by:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/477ad79cdb635cee87f6e7672d1bc3dc.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;You might have noticed that the origin of all prime rays is exactly (0, 0, 0).
This means that our camera is fixed at those coordinates. It is possible to adapt
this function to place the camera in different locations or orientations, but
we won&amp;rsquo;t need that for now.&lt;/p&gt;

&lt;h2 id=&#34;testing-for-intersections-with-the-sphere&#34;&gt;Testing for Intersections With The Sphere&lt;/h2&gt;

&lt;p&gt;Now that we have our prime rays, we need to know if they intersect with our
sphere. As usual, we&amp;rsquo;ll start with some definitions.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/5cb206e9d4f0dda63a44c1fa5d2908a2.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The basic idea behind this test is that we construct a right-triangle using the
prime ray as the adjacent side and the line between the origin and the center
of the sphere as the hypotenuse. Then we calculate the length of the opposite
side using the Pythagorean Theorem - if that side is smaller than the radius of
the sphere, the ray must intersect the sphere. In practice, we actually do the
check on length-squared values because square roots are expensive to calculate,
but it&amp;rsquo;s the same idea.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/sphere-intersection-test.png&#34; alt=&#34;Sphere Intersection Test&#34; /&gt;&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2fd3e237481614d13a34dc184cb5d106.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;finishing-the-render-method&#34;&gt;Finishing the Render Method&lt;/h2&gt;

&lt;p&gt;Now that we have all of the hard parts done, we simply need to integrate these
functions into the render function and produce our image:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/b2f715736405503985ef66f3732746c5.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;After adding some extra glue code to parse a scene definition and save the
rendered image to a file, we get the resulting image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://bheisler.github.io/static/raytracer-first-render.png&#34; alt=&#34;First Rendered Image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It isn&amp;rsquo;t very impressive yet, but we&amp;rsquo;ll add more detail to it as we go. In the
&lt;a href=&#34;https://bheisler.github.io/post/writing-raytracer-in-rust-part-2/&#34;&gt;next post&lt;/a&gt;,
we&amp;rsquo;ll add planes, multiple spheres, and some basic lighting effects.&lt;/p&gt;

&lt;p&gt;If you want to try playing around with the code yourself, you can check out the
&lt;a href=&#34;https://github.com/bheisler/raytracer&#34;&gt;GitHub Repository&lt;/a&gt;. If you want to learn
more about 3D rendering in general or raytracing in particular, check out
&lt;a href=&#34;https://www.scratchapixel.com/index.php&#34;&gt;Scratchapixel&lt;/a&gt;, which is the resource
I used while working on this.&lt;/p&gt;

&lt;p&gt;Thanks to Scott Olson and Daniel Hogan for suggesting improvements to an
earlier version of this article.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experiments In NES JIT Compilation</title>
      <link>https://bheisler.github.io/post/experiments-in-nes-jit-compilation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://bheisler.github.io/post/experiments-in-nes-jit-compilation/</guid>
      <description>

&lt;p&gt;Inspired by the always-incredible work on &lt;a href=&#34;https://dolphin-emu.org/&#34;&gt;Dolphin&lt;/a&gt;,
I decided to write myself an &lt;a href=&#34;https://github.com/bheisler/Corrosion&#34;&gt;NES emulator&lt;/a&gt;
called Corrosion a couple years ago. I managed to get it working well enough to
play basic games, and then put the project aside. This post is not about the
emulator itself, but rather the JIT compiler I added to it last year and the
upgrades to said JIT compiler I&amp;rsquo;ve made over the past few weeks.&lt;/p&gt;

&lt;p&gt;Having read that, you might be wondering &amp;ldquo;Why would anybody write a JIT compiler
for the NES?&amp;rdquo; Indeed, it&amp;rsquo;s a reasonable question. Unlike newer consoles, it&amp;rsquo;s
quite feasible to emulate the NES&amp;rsquo;s modified 6502 CPU at full speed with a
simple interpreter. As with most of the projects I write about here, I wanted
to know how they work, so I built one. Having done so, I can say that I would
not recommend JIT compilation for production-quality NES emulators except in
severely resource-constrained environments. However, I would strongly recommend
this project for anyone who wants to learn more about JIT compilers, as it&amp;rsquo;s
complex enough to be challenging but simple enough to be manageable.&lt;/p&gt;

&lt;p&gt;This is more of a post-mortem article covering the design of my JIT compiler,
the pitfalls I ran into and the mistakes I made in construction and what I&amp;rsquo;ve
learned from the process. It is not a tutorial on how to write your own JIT
compiler, though there are some links that cover that in more detail at the end.
The emulator is written in Rust, but you don&amp;rsquo;t need to know Rust to follow along.
Most of the concepts will map to other low-level languages like C or C++. An
understanding of x64 assembly would be helpful, but again, not required - I
didn&amp;rsquo;t know much assembly starting this project, and even now my assembly is
pretty weak.&lt;/p&gt;

&lt;h2 id=&#34;basics-of-jit-compilation&#34;&gt;Basics of JIT Compilation&lt;/h2&gt;

&lt;p&gt;Just to make sure everyone&amp;rsquo;s on the same page, a quick interlude on how JIT
compilers work at a high level. If you&amp;rsquo;re familiar with this already, feel free
to skip ahead.&lt;/p&gt;

&lt;p&gt;Broadly speaking, a JIT (or just-in-time) compiler is a piece of code that
translates some kind of program code into machine instructions for the host CPU.
The difference between a JIT compiler and a regular compiler is that a JIT
compiler performs this translation at runtime (hence just-in-time) rather than
compiling the code and saving a binary for later execution.
For emulation, the original program code is typically the binary machine code
that was intended for the emulated CPU (in this case the NES&amp;rsquo; 6502 CPU). However,
JIT compilers are used for many other kinds of programs. Examples include the
JIT compilers used by modern browsers to run Javascript, the Hotspot compiler
in the JVM and dynamic language runtimes like PyPy and LuaJIT.&lt;/p&gt;

&lt;p&gt;JIT compilers are used primarily to speed up execution. A standard interpreter
must fetch, decode and execute instructions one at a time. Even in a relatively
fast language like Rust or C, this incurs some overhead. A JIT compiler, on the
other hand, can be run once and emit a blob of machine code which executes an
entire emulated function (or more) in one sequence of instructions. Eliminating
that overhead often greatly improves execution speed. However, since the
compilation is done at runtime, care must be taken that the JIT compiler itself
doesn&amp;rsquo;t run slowly enough to cause performance problems, where an ahead-of-time
(AOT) compiler can spend much more time optimizing the code it generates.&lt;/p&gt;

&lt;p&gt;A JIT compiler typically parses some chunk of code, performs any analysis
it needs to, and then generates binary machine code for the host CPU into a
code buffer. Modern OS&amp;rsquo;s require these code buffers to be marked as read-only
and executable before they can be executed, but once this is done the generated
code can be executed by jumping the host processor to the beginning of the buffer
just like any normal function. Some more sophisticated JIT compilers will
translate the source language into some intermediate in-memory representation
for further processing before emitting the final machine code.&lt;/p&gt;

&lt;p&gt;As a simple example, consider the following 6502 code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;LDA $1A  // Load byte from RAM at 0x001A into A register
ADC #$20 // Add 0x20 to the A register
STA $1A  // Store A register into RAM at 0x001A
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This might be translated into the following (simplified) x64 code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MOV r9b, [rdx + 1Ah] // Load byte from RAM array pointed to by rdx into r9b
ADC r9b, 20h         // Add 0x20 to r9b, which represents the A register
MOV [rdx + 1Ah], r9b // Store the result back into the RAM array at 0x001A
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that I&amp;rsquo;ve omitted things like processor flags and interrupts from this
example.&lt;/p&gt;

&lt;h2 id=&#34;design-of-corrosion-s-jit&#34;&gt;Design of Corrosion&amp;rsquo;s JIT&lt;/h2&gt;

&lt;p&gt;Corrosion has a relatively simplistic JIT compiler. It has no intermediate
representation or register allocator, which might be found in more sophisticated
JIT compilers - Dolphin&amp;rsquo;s PPC JIT has a register allocator, while David Sharp&amp;rsquo;s
Tarmac ARM emulator features an IR called Armlets (see links at the end).
Since machine code is typically a binary format too complex for humans to write
directly, most JIT compilers also devote much code to translating some
assembly-like syntax or DSL used by the developers into the bytes that are given
to the host CPU. Fortunately for me, there is an extremely useful compiler plugin
by CensoredUsername called &lt;a href=&#34;https://github.com/CensoredUsername/dynasm-rs&#34;&gt;dynasm-rs&lt;/a&gt;
which can parse an Intel-assembly-like syntax and perform most of the assembly
at compile time. I would recommend any Rust-based JIT compiler author should
check out this plugin; I&amp;rsquo;ve found it to work well, with no bugs to speak of and
CensoredUsername was very helpful about answering my silly questions when I asked.
The only limitation is that it currently only supports the x64 instruction set,
though x86 support is planned. For those who prefer C/C++, there is a similar
tool called &lt;a href=&#34;https://luajit.org/dynasm_features.html&#34;&gt;DynASM&lt;/a&gt;, though I can&amp;rsquo;t
comment on that as I&amp;rsquo;ve never used it myself.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/a949bf7d08573e4529b4a9e2fd10f5e6.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The entry point to the JIT compiler in Corrosion is the
&lt;a href=&#34;https://github.com/bheisler/Corrosion/blob/develop/src/cpu/dispatcher.rs&#34;&gt;dispatcher module&lt;/a&gt;.
When the CPU interpreter detects that it&amp;rsquo;s executing an address from the ROM,
it makes a call to the dispatcher to compile (if necessary) and execute the
relevant block of code. The dispatcher is responsible for managing the cache
of generated code blocks and calling to the JIT compiler to generate more code
when necessary.&lt;/p&gt;

&lt;p&gt;If the dispatcher doesn&amp;rsquo;t have an existing generated code block for a particular
location in ROM, the &lt;a href=&#34;https://github.com/bheisler/Corrosion/blob/develop/src/cpu/nes_analyst.rs&#34;&gt;nes_analyst module&lt;/a&gt;
is used to collect information about the code to be compiled. The primary
responsibility of nes_analyst is to determine where the end of the current
function is and collect information about the instructions it contains.
This is done using a very simplistic algorithm that I copied from Dolphin. It
decodes instructions until it finds the first unconditional exit point (eg.
returns, jumps or calls to other functions). To ignore the conditional exit
points, it tracks the target address of the farthest forward-facing branch it&amp;rsquo;s
seen; any exit point before that is conditional. This approach does occasionally
overestimate the length of the actual function, but it&amp;rsquo;s simple and fast.
The nes_analyst module is also responsible for identifying which instructions
are the targets of branches and which instructions change or use which processor
flags, which is used later in the compilation process. Decoding opcodes is done
using the &lt;code&gt;decode_opcode!&lt;/code&gt; macro which expands to a giant match structure that
calls the appropriate functions. &lt;code&gt;decode_opcode!&lt;/code&gt; has handling for the various
addressing modes which we don&amp;rsquo;t really need here, so there is some clutter,
but it works well enough.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, Corrosion doesn&amp;rsquo;t have a register allocator. It&amp;rsquo;s quite
common for emulated CPU&amp;rsquo;s to have more registers than the host CPU, especially
since many JIT compilers run on the relatively register-light x86
and x64 instruction sets. As a result, they need to do the extra step of
determining which emulated registers should be represented by host registers
and which should be stored in memory at any given point in the code. Conveniently,
the NES&amp;rsquo;s 6502 CPU has even fewer registers than x64 does, which means we can
statically assign one x64 register to represent each 6502 register and have a
few left over to store things like the pointers to the Rust CPU structure and
the array which stores the emulated RAM, as well as a few more for general-purpose
scratch memory.&lt;/p&gt;

&lt;p&gt;Most 6502 instructions come in various different flavors called addressing modes,
which control where they take some of their data from. Take the CPX (ComPare X)
instruction as an example. This instruction compares the value in the X register
to a one-byte operand, setting the N (sign), Z (zero), and C (carry) flags.
If the opcode is 0xE0, the operand is a one-byte immediate value stored
right after the opcode. If the opcode is 0xE4, the next byte is instead
zero-extended to 16 bits and used as an address into RAM. This mode is called
the zero-page mode, and it can only access the first 255 bytes of RAM, which are
called the Zero Page. The byte at the selected location is used for the
comparison. Finally, if the opcode is 0xEC, the next two bytes (little-endian)
are used as an absolute address into memory and whichever byte they select is used.
If you&amp;rsquo;re wondering, zero page instructions are one byte smaller and slightly
faster than absolute instructions, which matters when you have a 64k address
space and 1.34MHz CPU.&lt;/p&gt;

&lt;p&gt;There are a number of other addressing modes, but this should suffice to explain
the concept. I could have written hand-tuned machine code for all 255 possible
opcodes, but I&amp;rsquo;m a lazy programmer, so instead I wrote a collection of routines
that generate code to move the appropriate byte into one of my scratch registers
(r8). That way, I can call the routine appropriate for the addressing mode to load
the operand into r8, then define the instruction code to take it from there.
Likewise, when writing to memory, I can move the value to be written into r8
and call a routine to generate the instructions to transfer that value into the
appropriate location in memory. It&amp;rsquo;s slightly less efficient at runtime because
I have to move data through an intermediate register instead of using it
directly, but it saved a lot of my time.&lt;/p&gt;

&lt;p&gt;Slight aside - I was a bit surprised by how small the difference is between
writing code to implement something and writing code that generates a program
to implement something. I&amp;rsquo;ll use CPX as an example again - this is some code
from an earlier version of the JIT:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/eebebbacefda3c626f597a6c865805dd.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;If I were actually writing this in assembly, this reads like pretty much how
I&amp;rsquo;d do it - call the function for the appropriate addressing mode to load the
operand, do some branching to set or clear the carry flag, compare the operand
against the X register and call some functions to update the sign and zero
flags. In fact, that&amp;rsquo;s exactly how the interpreter handles this instruction.
Instead, I&amp;rsquo;m calling a function to generate the code to load the operand,
generating code to do the comparison and update the flags, etc. Despite that
extra layer of indirection, though, it reads pretty much the same. Because
of this, implementing all of the instructions was as straightforward as
translating my Rust code into assembly. I&amp;rsquo;m not actually that good with
assembly, so my code will probably make experienced assembly programmers cry.
Still, it does the job. With that said, I would be interested in ideas for
making it better if anyone would care to share links or suggestions.&lt;/p&gt;

&lt;h2 id=&#34;enhancements&#34;&gt;Enhancements&lt;/h2&gt;

&lt;p&gt;That brings me up to the present, more or less. Over the past few weeks I&amp;rsquo;ve
been working on some &amp;lsquo;optimizations&amp;rsquo; to the JIT compiler. I write that in quotes
because for the most part I can&amp;rsquo;t actually detect any measurable change in
execution speed for these, but they were somewhat interesting to implement.&lt;/p&gt;

&lt;p&gt;The first such enhancement that I added was redundant flag elimination. This was
actually really easy and I probably should have done it from the start. The idea
here is that a good chunk of the code emitted by a JIT compiler (at least for
emulators) does nothing but implement the various flag behaviors of the emulated
CPU (eg. setting the overflow flag when an addition overflows). To some extent,
a clever JIT compiler author can exploit similar flags in the host CPU to
accomplish this with fewer instructions, but it&amp;rsquo;s still there. If you look at
&lt;a href=&#34;http://www.oxyron.de/html/opcodes02.html&#34;&gt;documents detailing the 6502&amp;rsquo;s instruction set&lt;/a&gt;,
you&amp;rsquo;ll quickly see that many instructions change the flags in some way,
but very few instructions use them. What this means is that a typical program
will overwrite processor flags far more often than they&amp;rsquo;re actually used.
Interpreters sometimes take advantage of this by not storing the flags at all,
and instead storing enough data to calculate the flags and then evaluating them
lazily when needed. A JIT compiler, however, can go one step further and analyze
every instruction to see if that flag value will be used before it&amp;rsquo;s overwritten
by another instruction. If not, it doesn&amp;rsquo;t emit the machine code to update the
flag.&lt;/p&gt;

&lt;p&gt;The way I implemented this is to have nes_analyst keep track of the last
instruction to change each flag while it&amp;rsquo;s stepping through a function. Then
when it hits an instruction that uses a flag, it looks up the InstructionAnalysis
structure for the last instruction to set the flag, which contains a set of
booleans indicating whether each flag will be used. Since we now know that that
instruction&amp;rsquo;s flag will be used and not overwritten, we set the appropriate
boolean to true, signaling the JIT compiler to emit code to update that flag
later on.&lt;/p&gt;

&lt;p&gt;There are a few pitfalls with this approach. For instance, if a branch is taken
or if execution hits a jump instruction, we can&amp;rsquo;t know if the code it jumps to
will rely on this flag. If so, this optimization could break. A more
sophisticated analysis could probably detect that, for at least some cases.
This one-pass algorithm can&amp;rsquo;t, so to be on the safe side it assumes that jump
and branch instructions use all of the flags. Likewise, when an interrupt
occurs, the NES pushes the flags and the return address on the stack. Since an
interrupt can occur at any time, there&amp;rsquo;s no way to be sure that the flags byte
it pushes on the stack will be correct. I don&amp;rsquo;t have a solution to this except
to assume that no game will break because of the exact value of the flags byte
on the stack. This seems like a safe assumption. Since interrupts can happen at
any time, it would be difficult to know what the flags should have looked like
when the interrupt happened. Something to be aware of, though.&lt;/p&gt;

&lt;p&gt;The initial version of my JIT compiler emitted a fixed series of instructions
(a function prologue) at the beginning of every compiled block which rearranged
the arguments from the win64 calling convention and loaded all of the NES
register values out of memory into the designated x64 registers. Then, at every
possible exit point from the block, it would emit some code (the epilogue) to
do the reverse; store the register values back in memory and return control
back to the interpreter. This means we can&amp;rsquo;t just jump to the middle of a
compiled function - we&amp;rsquo;ll skip over the prologue and crash. Therefore, if some
other code tries to jump into the middle of a function, we need to compile that
function suffix as a complete function of its own, with its own prologue and
epologues. Also, these duplicate prologues and epilogues take up space in the
instruction cache, which could reduce performance.&lt;/p&gt;

&lt;p&gt;Instead, I&amp;rsquo;ve changed it to use a trampoline; this is an ordinary Rust function
taking the pointer to the compiled code to jump to as well as the pointers to
the CPU structure and the RAM array. It contains an &lt;code&gt;asm!&lt;/code&gt; macro which defines
the assembly instructions to load the registers from memory, call the compiled
block and then store the updated registers back into memory. Since we now only
have one global &amp;lsquo;prologue/epilogue&amp;rsquo; shared between all compiled code blocks, we
can then call directly into the middle of an existing block with no trouble.&lt;/p&gt;

&lt;p&gt;Another problem with the prologue/epilogue design was that compiled blocks
couldn&amp;rsquo;t easily call each other; the JIT would have to store everything back in
memory to prepare for the prologue to be run again, or know how to jump past
the prologue or something else complicated. With a trampoline-based design,
it&amp;rsquo;s easy to jump to another block - everything&amp;rsquo;s already loaded into the
appropriate registers, so you can just jump the host processor to the beginning
of the target block. One wrinkle is that you need to be careful not to link
together blocks from different banks of ROM, since one bank could be switched
out and now your code is jumping to the wrong place.&lt;/p&gt;

&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;

&lt;p&gt;Speaking of that trampoline function, I did run into some difficulty implementing
it. The trampoline function needs to transfer values from a struct in memory
to and from registers. It takes a pointer to a CPU struct as an argument, but
that alone isn&amp;rsquo;t enough; Rust can rearrange and pad the fields however it likes,
so I needed a way to get the offset of each field from the pointer to the CPU.
C/C++ programmers can use the offsetof macro, but Rust has no official way to
calculate the offset of a field within a structure. The layout of Rust structures
isn&amp;rsquo;t even guaranteed to be the same from release to release - in fact, it &lt;a href=&#34;http://camlorn.net/posts/April%202017/rust-struct-field-reordering.html&#34;&gt;was
changed&lt;/a&gt;
just a few months ago in version 1.18.  I could have marked the CPU struct with
&lt;code&gt;repr(C)&lt;/code&gt; to force it to use the C layout and used hard-coded offsets, but that
felt inelegant. I would have needed to update the offsets every time I modified
the CPU struct, for one thing. Instead, I found a macro online that can calculate
the offset of any field in a structure.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/b21c9bfc07ee4c1afac2e96ef55dfffd.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;This works by casting 0 (NULL) to a raw pointer to a &lt;code&gt;$ty&lt;/code&gt; structure,
dereferencing it, taking a reference to the field and casting that pointer back
to a usize. As far as I can tell, this is actually safe and should be entirely
evaluated at compile time, but it still needs to be wrapped in an unsafe block
anyway. Use at your own risk, etc. etc. It&amp;rsquo;s pretty easy to add more macros to
calculate offsets with multiple levels of nesting - see &lt;code&gt;offset_of_2&lt;/code&gt; in
&lt;code&gt;x86_64_compiler/mod.rs&lt;/code&gt; for an example. One drawback of this is that it can&amp;rsquo;t
be used for static values - it&amp;rsquo;s forbidden to dereference null pointers when
initializing static values, even with unsafe. Because of that, I didn&amp;rsquo;t think
it would work with the &lt;code&gt;asm!&lt;/code&gt; macro&amp;rsquo;s &lt;code&gt;n&lt;/code&gt; value constraint (meaning constant
integer) but it totally does. Still, it&amp;rsquo;d be really nice if this was something
Rust supported out of the box.&lt;/p&gt;

&lt;p&gt;Another challenge I ran into while implementing this is dealing with some
quirks of the win64 calling convention. Rust, you see, does not have a defined
calling convention, so there&amp;rsquo;s no reliable way to call directly into Rust code
from assembly. Instead, you expose a function marked &lt;code&gt;extern &amp;quot;win64&amp;quot;&lt;/code&gt; or
similar which then calls the function you actually want. This way, you set up
your code to be compatible with the chosen calling convention - pushing
caller-saved registers on the stack, placing arguments in the right registers -
and leave Rust to handle the translation to its own internal calling
convention. The win64 convention is one of two 64-bit calling conventions
supported - the other one, sysv64, is still experimental and requires a special
feature flag even on nightly. The JIT compiler needs to call back into Rust
code to handle things like reading and writing memory-mapped devices like the
PPU or the controllers. Unfortunately, win64 is slightly difficult to work
with. It requires that the stack pointer be 16-byte aligned at the entry to
every function, and that the caller provide a 32-byte empty space on the stack
before the return address for the callee to use as scratch space. Failure to do
this correctly causes hard-to-debug segfaults. In my code, I don&amp;rsquo;t have many
places where I call back to Rust code, and the generated code doesn&amp;rsquo;t use the
stack very much, so I deal with this by just hard-coding the number of bytes of
space to leave on the stack. It&amp;rsquo;s not ideal (if I had more complex requirements
I might add a trampoline_to_win64 function to match trampoline_to_nes), but
other JIT compiler authors should be aware of it.&lt;/p&gt;

&lt;p&gt;Next up, debugging. Debugging a JIT compiler sucks even more than debugging an
interpreter. Debugging tools largely just don&amp;rsquo;t handle runtime-generated
machine code. Visual Studio, despite having a quite competent disassembly view,
just will not step into a generated code block. GDB&amp;rsquo;s disassembly view will at
least display the generated code and let you scroll downwards through it, but
not back upwards (I guess because it doesn&amp;rsquo;t know which byte to start
disasembling from, but it could at least allow you to scroll back up to the
program counter). GDB also fails to insert breakpoints into generated code
blocks even when you give it the address of the instruction to break at. GDB
has some sort of interface for exposing debugging info for JIT-compiled code,
but I wasn&amp;rsquo;t able to make much sense of it. Apparently it relies on the JIT
compiler generating and emitting a complete ELF table in memory for the
generated code, which sounds like a lot of hassle. Anyway, in the absence of a
debugger, good old println-debugging is your best friend. This is complicated
by the fact that you have to insert your debug output into the generated code
at runtime, but I&amp;rsquo;d strongly suggest you find a way. I wish I had done this
earlier, it would have saved me a ton of debugging time.&lt;/p&gt;

&lt;p&gt;Handling interrupts also proved to be something of a challenge. The NES has
very tight synchronization between the CPU and the other hardware, which
includes interrupts. I had hoped there would be some clever way to implement
interrupts without just checking if there had been an interrupt before every
emulated instruction, but I couldn&amp;rsquo;t find one. This is part of why the
duplicate epilogues were a problem, in fact; every emulated instruction was
preceded by an implicit exit point, so there were a lot of redundant epilogues.
The best I could come up with was to store a cycle count representing when the
next interrupt would occur and then compare that against the actual cycle count
before every instruction. This sort of works, because the hardware interrupts
of the NES are entirely predictable, but it probably wouldn&amp;rsquo;t work for other
systems. On the other hand, other systems probably don&amp;rsquo;t require such tight
timing for the interrupts, so if you&amp;rsquo;re writing a JIT you might be able to get
away with only checking for interrupts once every 10 emulated instructions or
something.&lt;/p&gt;

&lt;p&gt;As I mentioned in my post on &lt;a href=&#34;https://bheisler.github.io/post/nes-rom-parser-with-nom/&#34;&gt;parsing iNES ROM headers&lt;/a&gt;,
the NES only has 32k of address space mapped to the ROM. Some games take up
more than a megabyte of ROM space, so NES cartridges incorporate circuitry so
that the game can map banks of the ROM in and out of the address space.
Implementing the bankswitching logic is one thing, but this allows for the
possibility of self-modifying code even if you only use the JIT compiler when
executing from ROM. There are all sorts of wacky corner cases this enables -
what if the bank you&amp;rsquo;re executing is switched out between instructions? What if
half of a block is on one bank and the other is on the next bank, then the
second half gets switched out? If you then execute a generated code block that
compiled in the instructions from the original bank, the game will probably
break. You could even have a multi-byte instruction on a bank boundary, such
that the last byte of the instruction depends on which bank is mapped in. I&amp;rsquo;ll
be honest, I didn&amp;rsquo;t solve this problem. Corrosion just assumes that no game
will do strange stuff like this. Initially, I took a much more conservative
approach and deleted all of the compiled code for a bank whenever it was
switched out. This was a mistake; games like Legend of Zelda bankswitch
frequently enough that the emulator was constantly recompiling sections of code
that it had already compiled before. Major respect for the developers of other
JIT-based emulators - dealing with arbitrary self-modifying code, especially in
situations where you have an instruction cache and/or pipelining, must be a
nightmare.&lt;/p&gt;

&lt;h2 id=&#34;other-resources-conclusion&#34;&gt;Other resources &amp;amp; conclusion&lt;/h2&gt;

&lt;p&gt;Well, that&amp;rsquo;s about it from me. This was a bit more stream-of-consciousness than
my posts usually are, since I was writing about something I made a while ago.
I normally write my posts concurrent with working on the projects they cover.
I hope you found it interesting and/or educational. I&amp;rsquo;ll leave you with some
links to other resources that I used or wish that I&amp;rsquo;d known about when I was
building this thing.&lt;/p&gt;

&lt;p&gt;First off, Eli Bendersky&amp;rsquo;s Adventures In JIT Compilation series
(&lt;a href=&#34;http://eli.thegreenplace.net/2017/adventures-in-jit-compilation-part-1-an-interpreter/&#34;&gt;Part 1&lt;/a&gt;,
&lt;a href=&#34;http://eli.thegreenplace.net/2017/adventures-in-jit-compilation-part-2-an-x64-jit/&#34;&gt;Part 2&lt;/a&gt;,
&lt;a href=&#34;http://eli.thegreenplace.net/2017/adventures-in-jit-compilation-part-3-llvm/&#34;&gt;Part 3&lt;/a&gt;,
&lt;a href=&#34;http://eli.thegreenplace.net/2017/adventures-in-jit-compilation-part-4-in-python/&#34;&gt;Part 4&lt;/a&gt;)
is an excellent introduction to the low-level details of implementing an
interpreter and a series of JITs for Brainfuck, including different ways of
generating machine code, intermediate representations and so on.&lt;/p&gt;

&lt;p&gt;Second, David Sharp&amp;rsquo;s &lt;a href=&#34;http://www.davidsharp.com/tarmac/tarmacreport.pdf&#34;&gt;report on Tarmac&lt;/a&gt;,
an optimizing JIT compiler for ARM emulation. It&amp;rsquo;s over a hundred pages long,
but this is an excellent overview of JIT compilation techniques as well as a
detailed explanation of how Tarmac works. Sharp gives a good explanation (often
including diagrams and/or examples) of common approaches to various problems in
emulation, even if Tarmac itself doesn&amp;rsquo;t use them. If nothing else, read it to
learn about terminology you can plug into a search engine to find out more.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re interested in NES emulation in particular, &lt;a href=&#34;http://wiki.nesdev.com/w/index.php/Nesdev_Wiki&#34;&gt;the NESdev wiki&lt;/a&gt; is the premiere source of information for aspiring emulator developers and
homebrew ROM authors. This wiki and the resources it links to (including
&lt;a href=&#34;http://forums.nesdev.com/&#34;&gt;the forums&lt;/a&gt;,
&lt;a href=&#34;https://wiki.nesdev.com/w/index.php/Emulator_tests&#34;&gt;test ROMs&lt;/a&gt;, and lots
of documentation about the CPU/PPU/APU) provided all of the documentation I used
to build this emulator in the first place.&lt;/p&gt;

&lt;p&gt;Finally, Dolphin&amp;rsquo;s JIT doesn&amp;rsquo;t seem to have much documentation, so if you want
to find out more about it there are only two sources that I&amp;rsquo;ve found useful.
The &lt;a href=&#34;https://github.com/dolphin-emu/dolphin/tree/master/Source/Core/Core/PowerPC&#34;&gt;source code&lt;/a&gt;,
and &lt;a href=&#34;https://www.reddit.com/r/emulation/comments/2xq5ar/how_close_is_dolphin_to_being_cycle_accurate/cp318ka/&#34;&gt;this Reddit comment&lt;/a&gt;
by one of the developers giving a relatively high-level overview of how it all
works.&lt;/p&gt;

&lt;p&gt;This has been a fun project. I have some other stuff in the pipeline at the
moment, but I&amp;rsquo;d like to come back to this emulator at some point. Until next
time&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>