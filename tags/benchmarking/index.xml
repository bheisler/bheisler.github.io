<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Benchmarking on bheisler.github.io</title>
    <link>https://bheisler.github.io/tags/benchmarking/index.xml</link>
    <description>Recent content in Benchmarking on bheisler.github.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://bheisler.github.io/tags/benchmarking/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Criterion.rs v0.2 - HTML, Throughput Measurements, API Changes</title>
      <link>https://bheisler.github.io/post/criterion-rs-0-2/</link>
      <pubDate>Mon, 05 Feb 2018 07:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/criterion-rs-0-2/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;m pleased to announce the release of Criterion.rs v0.2, available today. Version 0.2 provides a
number of new features including HTML reports and throughput measurements, fixes a handful of bugs,
and adds a new, more powerful way to configure and construct your benchmarks. It also breaks
backwards compatibility with the 0.1 versions in a number of small but important ways. Read on to
learn more!&lt;/p&gt;

&lt;h2 id=&#34;what-is-criterion-rs&#34;&gt;What is Criterion.rs?&lt;/h2&gt;

&lt;p&gt;Criterion.rs is a statistics-driven benchmarking library for Rust. It provides precise measurements
of changes in the performance of benchmarked code, and gives strong statistical confidence that
apparent performance changes are real and not simply noise. Clear output, a simple API and
reasonable defaults make it easy to use even for developers without a background in statistics.
Unlike the benchmarking harness provided by Rust, Criterion.rs can be used with stable versions of
the compiler.&lt;/p&gt;

&lt;p&gt;If you aren&amp;rsquo;t already using Criterion.rs for your benchmarks, check out the &lt;a href=&#34;https://japaric.github.io/criterion.rs/book/getting_started.html&#34;&gt;Getting Started
guide&lt;/a&gt; or go right to &lt;a href=&#34;https://github.com/japaric/criterion.rs&#34;&gt;the GitHub
repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;new-features&#34;&gt;New Features&lt;/h2&gt;

&lt;p&gt;This is only some of the improvements made to Criterion.rs in v0.2 - for a more complete list, see
the &lt;a href=&#34;https://github.com/japaric/criterion.rs/blob/master/CHANGELOG.md&#34;&gt;CHANGELOG&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;html-reports&#34;&gt;HTML Reports&lt;/h3&gt;

&lt;p&gt;Criterion.rs now generates an HTML report for each benchmark, including detailed graphs showing the
performance behavior of your code. For an example of the generated report, &lt;a href=&#34;https://japaric.github.io/criterion.rs/book/user_guide/html_report/index.html&#34;&gt;click
here&lt;/a&gt;.
&lt;a href=&#34;http://www.gnuplot.info/&#34;&gt;Gnuplot&lt;/a&gt; must be installed in order to generate reports.&lt;/p&gt;

&lt;p&gt;The reports and other data are now stored in the &lt;code&gt;target/criterion&lt;/code&gt; directory when you run the
benchmarks, which makes them easier to find and means you no longer need to ignore the &lt;code&gt;.criterion&lt;/code&gt;
directory.&lt;/p&gt;

&lt;p&gt;There is still much work to do on expanding the HTML reports, so stay tuned for further enhancements.&lt;/p&gt;

&lt;h3 id=&#34;criterion-bench&#34;&gt;Criterion.bench&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://japaric.github.io/criterion.rs/criterion/struct.Criterion.html#method.bench&#34;&gt;&lt;code&gt;bench&lt;/code&gt;&lt;/a&gt;
function has been added to the &lt;code&gt;Criterion&lt;/code&gt; struct, along with two new structures -
&lt;a href=&#34;https://japaric.github.io/criterion.rs/criterion/struct.Benchmark.html&#34;&gt;&lt;code&gt;Benchmark&lt;/code&gt;&lt;/a&gt; and
&lt;a href=&#34;https://japaric.github.io/criterion.rs/criterion/struct.ParameterizedBenchmark.html&#34;&gt;&lt;code&gt;ParameterizedBenchmark&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt;.
These structures provide a powerful builder-style interface to define and configure complex
benchmarks which can perform benchmarks and comparisons that were not possible previously, such as
comparing the performance of a Rust function and an external program over a range of inputs. These
structs also allow for easy per-benchmark configuration of measurement times and other settings.&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;c.bench(
    &amp;quot;Fibonacci&amp;quot;,
    Benchmark::new(&amp;quot;Recursive&amp;quot;, |b| b.iter(|| fibonacci_recursive(20)))
        .with_function(&amp;quot;Iterative&amp;quot;, |b| b.iter(|| fibonacci_iterative(20))),
);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;throughput-measurements&#34;&gt;Throughput Measurements&lt;/h3&gt;

&lt;p&gt;Criterion.rs can now estimate the throughput of the code under test. By providing a
&lt;a href=&#34;https://japaric.github.io/criterion.rs/criterion/enum.Throughput.html&#34;&gt;&lt;code&gt;Throughput&lt;/code&gt;&lt;/a&gt; (for
&lt;code&gt;Benchmark&lt;/code&gt;) or &lt;code&gt;Fn(&amp;amp;T) -&amp;gt; Throughput&lt;/code&gt; (for &lt;code&gt;ParameterizedBenchmark&amp;lt;T&amp;gt;&lt;/code&gt;), you can tell Criterion.rs
how many bytes or elements are being processed in each iteration of your benchmark. Criterion.rs
will then use that information to estimate the number of bytes or elements your code can process per
second.&lt;/p&gt;

&lt;h2 id=&#34;breaking-changes&#34;&gt;Breaking Changes&lt;/h2&gt;

&lt;p&gt;Unfortunately, some breaking changes were necessary to implement these new features.&lt;/p&gt;

&lt;h3 id=&#34;builder-methods-take-self-by-value&#34;&gt;Builder Methods Take self by Value&lt;/h3&gt;

&lt;p&gt;All of the builder methods on &lt;code&gt;Criterion&lt;/code&gt; now take &lt;code&gt;self&lt;/code&gt; by value rather than by mutable reference.
This is to simplify chaining multiple methods after calling &lt;code&gt;Criterion::default()&lt;/code&gt;, but existing
code which configures a &lt;code&gt;Criterion&lt;/code&gt; structure may need to be changed or replaced with code that
configures a &lt;code&gt;Benchmark&lt;/code&gt; instead.&lt;/p&gt;

&lt;h3 id=&#34;static-lifetime-for-closure-types&#34;&gt;&amp;lsquo;static Lifetime For Closure Types&lt;/h3&gt;

&lt;p&gt;Most closures passed to Criterion.rs must now have types that live for the &lt;code&gt;&#39;static&lt;/code&gt; lifetime. Note,
the closures themselves don&amp;rsquo;t need to be &lt;code&gt;&#39;static&lt;/code&gt;, but their types do.&lt;/p&gt;

&lt;p&gt;What does this mean for you? You may need to change your benchmarks from &lt;code&gt;|b| b.iter(...)&lt;/code&gt; to
&lt;code&gt;move |b| b.iter(...)&lt;/code&gt;. This does mean that the closures will take ownership of values used inside
the closure, so you may need to clone or &lt;code&gt;Rc&lt;/code&gt; shared test data. Simple closures, like those in the
Fibonacci example above, can remain unchanged - this only affects closures which capture values
from their environment.&lt;/p&gt;

&lt;h3 id=&#34;benchmark-parameters-must-implement-debug&#34;&gt;Benchmark Parameters Must Implement Debug&lt;/h3&gt;

&lt;p&gt;Previously, Criterion.rs required the values for parameterized benchmarks to implement the &lt;code&gt;Display&lt;/code&gt;
trait. This has been changed to require the &lt;code&gt;Debug&lt;/code&gt; trait instead, as that can be easily derived.&lt;/p&gt;

&lt;h2 id=&#34;thank-you&#34;&gt;Thank You&lt;/h2&gt;

&lt;p&gt;Thank you to Damien Levac (@Proksima), @dowwie, Oliver Mader (@b52), Nick Babcock (@nickbabcock),
Steven Fackler (@sfackler), and @llogiq for suggesting improvements to Criterion.rs since the last
release. I&amp;rsquo;d also like to thank Alexander Bulaev (@alexbool) and Paul Mason (@paupino) for
contributing pull requests.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;d like to see your name up here, or if you have ideas, problems or questions, please consider
contributing to Criterion.rs on &lt;a href=&#34;https://github.com/japaric/criterion.rs&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking In Stable Rust With Criterion.rs</title>
      <link>https://bheisler.github.io/post/benchmarking-with-criterion-rs/</link>
      <pubDate>Fri, 12 Jan 2018 19:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/benchmarking-with-criterion-rs/</guid>
      <description>

&lt;p&gt;When I initially announced the release of Criterion.rs, I didn&amp;rsquo;t expect that
there would be so much demand for benchmarking on stable Rust. Now, I&amp;rsquo;d like to
announce the release of Criterion.rs 0.1.2, which supports the stable compiler.
This post is an introduction to benchmarking with Criterion.rs and a discussion
of reasons why you might or might not want to do so.&lt;/p&gt;

&lt;h2 id=&#34;what-is-criterion-rs&#34;&gt;What is Criterion.rs?&lt;/h2&gt;

&lt;p&gt;Criterion.rs is a benchmarking library for Rust that aims to bring solid
statistical confidence to benchmarking Rust code, while maintaining good
ease-of-use, even for programmers without a background in statistics. It&amp;rsquo;s
already available on &lt;a href=&#34;https://crates.io/crates/criterion&#34;&gt;Crates.io&lt;/a&gt; and on
&lt;a href=&#34;https://github.com/japaric/criterion.rs&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It was originally written by &lt;a href=&#34;https://github.com/japaric/&#34;&gt;@japaric&lt;/a&gt;, but was
never released on Crates.io. I (&lt;a href=&#34;https://github.com/bheisler&#34;&gt;@bheisler&lt;/a&gt;)
volunteered to take over maintenance and development a few months ago, and I
published the first version of Criterion.rs to Crates.io in December 2017.&lt;/p&gt;

&lt;h2 id=&#34;getting-started-with-criterion-rs&#34;&gt;Getting Started with Criterion.rs&lt;/h2&gt;

&lt;p&gt;To start with Criterion.rs, add the following to your &lt;code&gt;Cargo.toml&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[dev-dependencies]
criterion = &amp;quot;0.1.2&amp;quot;

[[bench]]
name = &amp;quot;my_benchmark&amp;quot;
harness = false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, define a benchmark by creating a file at &lt;code&gt;$PROJECT/benches/my_benchmark.rs&lt;/code&gt; with the following contents.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2da10e7aaac3011ce1d6328e3a4ffdce.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Finally, run this benchmark with &lt;code&gt;cargo bench&lt;/code&gt;. You should see output similar to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     Running target/release/deps/example-423eedc43b2b3a93
fib 20                  time:   [26.029 us 26.251 us 26.505 us]
Found 11 outliers among 99 measurements (11.11%)
  6 (6.06%) high mild
  5 (5.05%) high severe
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See the &lt;a href=&#34;https://japaric.github.io/criterion.rs/book/getting_started.html&#34;&gt;Getting Started&lt;/a&gt; guide for more details.&lt;/p&gt;

&lt;h2 id=&#34;converting-libtest-benchmarks-to-criterion-rs&#34;&gt;Converting libtest benchmarks to Criterion.rs&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll start with this benchmark as an example:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/61efe654cf235acab9966f8e3e55a5c3.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The first thing to do is update the &lt;code&gt;Cargo.toml&lt;/code&gt; to disable the libtest
benchmark harness:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[[bench]]
name = &amp;quot;example&amp;quot;
harness = false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to update the imports:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;#[macro_use]
extern crate criterion;
use criterion::Criterion;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we can change the &lt;code&gt;bench_fib&lt;/code&gt; function. Remove the &lt;code&gt;#[bench]&lt;/code&gt; and change
the argument to &lt;code&gt;&amp;amp;mut Criterion&lt;/code&gt; instead. The contents of this function need to
change as well:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;fn bench_fib(c: &amp;amp;mut Criterion) {
    c.bench_function(&amp;quot;fib 20&amp;quot;, |b| b.iter(|| fibonacci(20)));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we need to invoke some macros to generate a main function, since we
no longer have libtest to provide one:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;criterion_group!(benches, bench_fib);
criterion_main!(benches);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it! The complete migrated benchmark code is below:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/45675855d119ad6f03fa94a5247466fe.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;the-pitch-why-you-might-want-to-use-criterion-rs&#34;&gt;The Pitch - Why You Might Want to Use Criterion.rs&lt;/h2&gt;

&lt;p&gt;There are a number of reasons to use Criterion.rs.&lt;/p&gt;

&lt;p&gt;The biggest one, the one that drew me to it in the first place, is the
statistical confidence it provides. libtest gives a number and a confidence
interval of some sort, but I cant&amp;rsquo;t even tell if that number is higher or
lower than it was the last time I ran the benchmarks. Even if it is, how could
I tell if that change was due to random noise or a change in the performance of
the code? I&amp;rsquo;ve used Criterion.rs to benchmark and optimize my own projects and
every time I&amp;rsquo;ve seen it show a statistically-significant optimization or
regression it&amp;rsquo;s been real. It&amp;rsquo;s almost fun, tweaking the code and running the
benchmarks to see what happened. I&amp;rsquo;ve never gotten into that sort of flow with
libtest.&lt;/p&gt;

&lt;p&gt;Another big reason is that Criterion.rs is actively maintained and developed.
libtest is not, and the description of the bencher crate on GitHub declares
that new features will not be added. Indeed, it instructs the reader to &amp;ldquo;Go
build a better stable benchmarking library.&amp;rdquo; I hope Criterion.rs is that
library.&lt;/p&gt;

&lt;p&gt;Criterion.rs produces more statistical information than libtest, and generates
helpful charts and graphs to make it more easily understandable to the user.
Additionally, it automatically compares the results of one run with the
previous, without needing to install cargo-benchcmp or manually save benchmark
results to files.&lt;/p&gt;

&lt;p&gt;Finally, Criterion.rs is compatible with stable builds of Rust, where libtest is
not.&lt;/p&gt;

&lt;h2 id=&#34;the-anti-pitch-why-you-might-prefer-libtest&#34;&gt;The Anti-Pitch - Why You Might Prefer libtest&lt;/h2&gt;

&lt;p&gt;With all that said, I would also like to explain some reasons why Criterion.rs
might not be right for everyone.&lt;/p&gt;

&lt;p&gt;For example, libtest benchmarks execute much more quickly than
Criterion.rs benchmarks, especially the small and fast benchmarks. A small
libtest benchmark function can run to completion in less than a second, where
Criterion runs for (by default) at least 8 seconds plus analysis time. If your
project lends itself to many small benchmarks, you&amp;rsquo;d need to configure
Criterion.rs to run shorter tests, where you wouldn&amp;rsquo;t with libtest.&lt;/p&gt;

&lt;p&gt;The corollary to active development is that Criterion.rs&amp;rsquo; API is not yet fully
stablized, where libtest isn&amp;rsquo;t likely to change.&lt;/p&gt;

&lt;p&gt;libtest is also more seamless to use than Criterion.rs. You don&amp;rsquo;t need to mess
around with your &lt;code&gt;Cargo.toml&lt;/code&gt; file to use libtest benchmarks, they just work.
Along the same lines, libtest has the &lt;code&gt;test::black_box&lt;/code&gt; function to prevent
unwanted constant folding, which Criterion.rs can only approximate for now.
Finally, libtest is the only option for benchmarks within your main crate -
both Criterion.rs and bencher can only be used in the &lt;code&gt;benches&lt;/code&gt; folder at
present.&lt;/p&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;I hope I&amp;rsquo;ve convinced you to give Criterion.rs a look. I&amp;rsquo;m excited for the
future of this project and of Rust as a whole, and I hope you are too.&lt;/p&gt;

&lt;p&gt;Although Criterion.rs now supports stable Rust, that doesn&amp;rsquo;t mean that it
itself is stable, or even feature-complete. I certainly plan to continue
polishing and expanding on what Criterion.rs already provides. If you&amp;rsquo;d like to
help with that effort, or if you&amp;rsquo;d like to make suggestions, feature requests
or bug reports, please check out &lt;a href=&#34;https://github.com/japaric/criterion.rs&#34;&gt;the repository on
GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In addition, I hope to work with the Rust team to help define and implement the
necessary changes to Cargo and rustc to use alternate test and benchmark
frameworks. This would make it as seamless to use Criterion.rs as it already is
to use libtest, and will hopefully allow the community to experiment with a
variety of ways to support testing and benchmarking.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are Benchmarks From Cloud CI Services Reliable?</title>
      <link>https://bheisler.github.io/post/benchmarking-in-the-cloud/</link>
      <pubDate>Sat, 06 Jan 2018 16:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/benchmarking-in-the-cloud/</guid>
      <description>

&lt;p&gt;After I released the first version of &lt;a href=&#34;https://github.com/japaric/criterion.rs&#34;&gt;Criterion.rs&lt;/a&gt;,
(a statistics-driven benchmarking tool for Rust) I was asked about using it to
detect performance regressions as part of a cloud-based continuous integration
(CI) pipeline such as Travis-CI or Appveyor. That got me wondering - does it
even make sense to do that?&lt;/p&gt;

&lt;p&gt;Cloud-CI pipelines have a lot of potential to introduce noise into the benchmarking
process - unpredictable load on the physical hosts of the build VM&amp;rsquo;s, or
even unpredictable migrations of VMs between physical hosts. How much
noise is there really, and how much does it affect real-world benchmarks? I
couldn&amp;rsquo;t find any attempt to answer that question with real data, so I decided
to do it myself.&lt;/p&gt;

&lt;p&gt;tl;dr: Yes, there is enough noise to make benchmark results unreliable.
Read on if you want to see the numbers.&lt;/p&gt;

&lt;p&gt;In this post, I benchmarked on Travis-CI, but I don&amp;rsquo;t mean to single them out,
they&amp;rsquo;re just the cloud-CI provider that I&amp;rsquo;m most familiar with.
To the best of my knowledge, they don&amp;rsquo;t claim that their service is suitable
for benchmarking.&lt;/p&gt;

&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;Before I can test the effects of the cloud-CI environment on benchmarks, I need
some benchmarks. I opted to use the existing benchmark suite of Rust&amp;rsquo;s
&lt;a href=&#34;https://github.com/rust-lang/regex&#34;&gt;regex library&lt;/a&gt;, because it&amp;rsquo;s a well-known,
well-regarded and high-performance codebase. Specifically, I used the &amp;ldquo;rust&amp;rdquo;
benchmark suite. The regex project&amp;rsquo;s benchmarks use Rust&amp;rsquo;s standard &lt;a href=&#34;https://github.com/rust-lang/rust/tree/master/src/libtest&#34;&gt;&lt;code&gt;libtest&lt;/code&gt;
benchmark/test harness&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I ran the benchmarks in pairs, as suggested in &lt;a href=&#34;https://beachape.com/blog/2016/11/02/rust-performance-testing-on-travis-ci/&#34;&gt;this post by BeachApe&lt;/a&gt;.
However, that post suggests running one benchmark with master and one with a
pull-request branch - all of my benchmarks were done with the same version of the
code to prevent changes to the code from affecting the results. For the cloud
benchmarks, each pair was run in a separate build job on Travis-CI.&lt;/p&gt;

&lt;p&gt;I wrote a script to run 100 such pairs of builds on an old desktop machine I
had laying around, and another to run 100 Travis-CI builds by editing, committing
and force-pushing an unused file, then downloading the resulting build log.
Note that I did edit the Travis build script to only perform the necessary
compilation and benchmarking, to avoid using more of Travis-CI&amp;rsquo;s resources than
was necessary. A few of the resulting log files were damaged and were replaced
with log files from new builds at the end. There were a number of occasions where
parts of the logs from Travis-CI were missing or corrupted and I am not certain
that I found all of them.&lt;/p&gt;

&lt;p&gt;Each pair was then compared using &lt;a href=&#34;https://github.com/BurntSushi/cargo-benchcmp&#34;&gt;cargo benchcmp&lt;/a&gt;
and the percentage differences were extracted with more scripts.&lt;/p&gt;

&lt;p&gt;The pairwise benchmarking approach has a few advantages. First, by running both
benchmarks on the same physical machine (for local benchmarks) or the same
build job (for cloud benchmarks), all effects which are constant for the length
of a benchmark pair can be ignored. This includes differences in the performance
of the physical hosts or differences in compiler versions, since we&amp;rsquo;re only
looking at the percentage change between two consecutive benchmarks. Using the
percentage differences also controls for some benchmarks naturally taking longer
than others.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://bheisler.github.io/static/travis_benchmark_histogram.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;As you can see from the above chart, the cloud benchmarks do indeed show more
noise than the local benchmarks.&lt;/p&gt;

&lt;p&gt;All numbers are in units of percentage points representing the percentage
difference between the two benchmarks of a pair:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Local:
mean: 0.0315897959184
min: -24.6, max: 22.18
std. dev.: 2.11411179379

Cloud:
mean: 1.42961542492
min: -99.99, max: 3177.03
std. dev.: 72.1539676978

Levene&#39;s Test p-value: 1.97E-49
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that there were four benchmark results in the cloud set with percentage
differences greater than 10,000% which I&amp;rsquo;ve removed as outliers. Those were not
included in the calculations above; if they were included the cloud numbers would be
substantially worse. I opted to remove them after inspecting them and finding
inconsistencies in those benchmark results which lead me to suspect that the
logs were damaged. For example, one benchmark shows the time for each iteration
increased by more than 200x but the throughput for the same benchmark
appears to have increased slightly, rather than decreased as one would expect.&lt;/p&gt;

&lt;p&gt;Additionally, manual inspection of the comparison results shows that sometimes
multiple consecutive benchmark tests within a single run of the benchmarks
all differ from their pair by a large and consistent value. This could indicate
something is slowing down the build VM by a significant degree and persisting
long enough to affect multiple benchmark tests.&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;The greatly increased variance of benchmarks done in the cloud casts doubt on
the reliability of benchmarks performed on cloud-CI pipelines. This confirms the
intuitive expectation.&lt;/p&gt;

&lt;p&gt;To be clear; this doesn&amp;rsquo;t mean every benchmark is wrong - many of the comparisons
show shifts of +-2%, roughly similar to the noise observed in local benchmarks.
However, differences of as much as 50% are fairly common with no change
in the code at all, which makes it very difficult to know if a change
in benchmarking results is due to a change in the true performance of the code
being benchmarked, or if it is simply noise. Hence, unreliable.&lt;/p&gt;

&lt;p&gt;It would still be useful to have automated detection of performance regressions
as part of a CI pipeline, however. Further work is needed to find ways to
mitigate the effects of this noise.&lt;/p&gt;

&lt;p&gt;One way to reduce noise in this system would be to execute each benchmark suite
two or more times with each version of the code and accept the one with the
smallest mean or variance before comparing the two. In this case, it would be
best to run each benchmark suite to completion before running it again rather
than running each test twice consecutively, to reduce the chance that some
external influence affects a single test twice.&lt;/p&gt;

&lt;p&gt;A simpler, though more manual, method to accomplish the same thing would be to
run the whole benchmarking process in multiple build jobs. In that case, before
merging a pull request, a maintainer could manually examine the results. If a
performance regression is detected by all of the build jobs, it&amp;rsquo;s probably safe
to treat it as real rather than noise.&lt;/p&gt;

&lt;p&gt;It is also possible that different cloud-CI providers could make for less noisy
benchmarking environments, though I haven&amp;rsquo;t measured that.&lt;/p&gt;

&lt;p&gt;All of the data and analysis scripts can be found &lt;a href=&#34;https://github.com/bheisler/travis-benchmark-data&#34;&gt;on GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you to Daniel Hogan, for reading over this post and giving me a great deal
of useful feedback. I&amp;rsquo;d also like to thank Andrew Gallant (@burntsushi) and co.
for creating both the regex crate and cargo-benchcmp.&lt;/p&gt;

&lt;h2 id=&#34;addendum-why-libtest-and-not-criterion-rs&#34;&gt;Addendum: Why libtest and not Criterion.rs&lt;/h2&gt;

&lt;p&gt;I opted to use Rust&amp;rsquo;s standard benchmarking tool rather than Criterion.rs because
there are no large, well-regarded projects using Criterion.rs to perform their
benchmarks at present.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t know whether using Criterion.rs would change these results or not.
Criterion&amp;rsquo;s analysis process is different enough that it might, but until I have
data one way or another I intend to advise users not to trust cloud benchmarks
based on Criterion.rs.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>