<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Benchmarking on bheisler.github.io</title>
    <link>https://bheisler.github.io/tags/benchmarking/index.xml</link>
    <description>Recent content in Benchmarking on bheisler.github.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://bheisler.github.io/tags/benchmarking/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Benchmarking In Stable Rust With Criterion.rs</title>
      <link>https://bheisler.github.io/post/benchmarking-with-criterion-rs/</link>
      <pubDate>Fri, 12 Jan 2018 19:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/benchmarking-with-criterion-rs/</guid>
      <description>

&lt;p&gt;When I initially announced the release of Criterion.rs, I didn&amp;rsquo;t expect that
there would be so much demand for benchmarking on stable Rust. Now, I&amp;rsquo;d like to
announce the release of Criterion.rs 0.1.2, which supports the stable compiler.
This post is an introduction to benchmarking with Criterion.rs and a discussion
of reasons why you might or might not want to do so.&lt;/p&gt;

&lt;h2 id=&#34;what-is-criterion-rs&#34;&gt;What is Criterion.rs?&lt;/h2&gt;

&lt;p&gt;Criterion.rs is a benchmarking library for Rust that aims to bring solid
statistical confidence to benchmarking Rust code, while maintaining good
ease-of-use, even for programmers without a background in statistics. It&amp;rsquo;s
already available on &lt;a href=&#34;https://crates.io/crates/criterion&#34;&gt;Crates.io&lt;/a&gt; and on
&lt;a href=&#34;https://github.com/japaric/criterion.rs&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It was originally written by &lt;a href=&#34;https://github.com/japaric/&#34;&gt;@japaric&lt;/a&gt;, but was
never released on Crates.io. I (&lt;a href=&#34;https://github.com/bheisler&#34;&gt;@bheisler&lt;/a&gt;)
volunteered to take over maintenance and development a few months ago, and I
published the first version of Criterion.rs to Crates.io in December 2017.&lt;/p&gt;

&lt;h2 id=&#34;getting-started-with-criterion-rs&#34;&gt;Getting Started with Criterion.rs&lt;/h2&gt;

&lt;p&gt;To start with Criterion.rs, add the following to your &lt;code&gt;Cargo.toml&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[dev-dependencies]
criterion = &amp;quot;0.1.2&amp;quot;

[[bench]]
name = &amp;quot;my_benchmark&amp;quot;
harness = false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, define a benchmark by creating a file at &lt;code&gt;$PROJECT/benches/my_benchmark.rs&lt;/code&gt; with the following contents.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/2da10e7aaac3011ce1d6328e3a4ffdce.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Finally, run this benchmark with &lt;code&gt;cargo bench&lt;/code&gt;. You should see output similar to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     Running target/release/deps/example-423eedc43b2b3a93
fib 20                  time:   [26.029 us 26.251 us 26.505 us]
Found 11 outliers among 99 measurements (11.11%)
  6 (6.06%) high mild
  5 (5.05%) high severe
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See the &lt;a href=&#34;https://japaric.github.io/criterion.rs/book/getting_started.html&#34;&gt;Getting Started&lt;/a&gt; guide for more details.&lt;/p&gt;

&lt;h2 id=&#34;converting-libtest-benchmarks-to-criterion-rs&#34;&gt;Converting libtest benchmarks to Criterion.rs&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll start with this benchmark as an example:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/61efe654cf235acab9966f8e3e55a5c3.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The first thing to do is update the &lt;code&gt;Cargo.toml&lt;/code&gt; to disable the libtest
benchmark harness:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[[bench]]
name = &amp;quot;example&amp;quot;
harness = false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to update the imports:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;#[macro_use]
extern crate criterion;
use criterion::Criterion;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we can change the &lt;code&gt;bench_fib&lt;/code&gt; function. Remove the &lt;code&gt;#[bench]&lt;/code&gt; and change
the argument to &lt;code&gt;&amp;amp;mut Criterion&lt;/code&gt; instead. The contents of this function need to
change as well:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;fn bench_fib(c: &amp;amp;mut Criterion) {
    c.bench_function(&amp;quot;fib 20&amp;quot;, |b| b.iter(|| fibonacci(20)));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we need to invoke some macros to generate a main function, since we
no longer have libtest to provide one:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;criterion_group!(benches, bench_fib);
criterion_main!(benches);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it! The complete migrated benchmark code is below:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/bheisler/45675855d119ad6f03fa94a5247466fe.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;the-pitch-why-you-might-want-to-use-criterion-rs&#34;&gt;The Pitch - Why You Might Want to Use Criterion.rs&lt;/h2&gt;

&lt;p&gt;There are a number of reasons to use Criterion.rs.&lt;/p&gt;

&lt;p&gt;The biggest one, the one that drew me to it in the first place, is the
statistical confidence it provides. libtest gives a number and a confidence
interval of some sort, but I cant&amp;rsquo;t even tell if that number is higher or
lower than it was the last time I ran the benchmarks. Even if it is, how could
I tell if that change was due to random noise or a change in the performance of
the code? I&amp;rsquo;ve used Criterion.rs to benchmark and optimize my own projects and
every time I&amp;rsquo;ve seen it show a statistically-significant optimization or
regression it&amp;rsquo;s been real. It&amp;rsquo;s almost fun, tweaking the code and running the
benchmarks to see what happened. I&amp;rsquo;ve never gotten into that sort of flow with
libtest.&lt;/p&gt;

&lt;p&gt;Another big reason is that Criterion.rs is actively maintained and developed.
libtest is not, and the description of the bencher crate on GitHub declares
that new features will not be added. Indeed, it instructs the reader to &amp;ldquo;Go
build a better stable benchmarking library.&amp;rdquo; I hope Criterion.rs is that
library.&lt;/p&gt;

&lt;p&gt;Criterion.rs produces more statistical information than libtest, and generates
helpful charts and graphs to make it more easily understandable to the user.
Additionally, it automatically compares the results of one run with the
previous, without needing to install cargo-benchcmp or manually save benchmark
results to files.&lt;/p&gt;

&lt;p&gt;Finally, Criterion.rs is compatible with stable builds of Rust, where libtest is
not.&lt;/p&gt;

&lt;h2 id=&#34;the-anti-pitch-why-you-might-prefer-libtest&#34;&gt;The Anti-Pitch - Why You Might Prefer libtest&lt;/h2&gt;

&lt;p&gt;With all that said, I would also like to explain some reasons why Criterion.rs
might not be right for everyone.&lt;/p&gt;

&lt;p&gt;For example, libtest benchmarks execute much more quickly than
Criterion.rs benchmarks, especially the small and fast benchmarks. A small
libtest benchmark function can run to completion in less than a second, where
Criterion runs for (by default) at least 8 seconds plus analysis time. If your
project lends itself to many small benchmarks, you&amp;rsquo;d need to configure
Criterion.rs to run shorter tests, where you wouldn&amp;rsquo;t with libtest.&lt;/p&gt;

&lt;p&gt;The corollary to active development is that Criterion.rs&amp;rsquo; API is not yet fully
stablized, where libtest isn&amp;rsquo;t likely to change.&lt;/p&gt;

&lt;p&gt;libtest is also more seamless to use than Criterion.rs. You don&amp;rsquo;t need to mess
around with your &lt;code&gt;Cargo.toml&lt;/code&gt; file to use libtest benchmarks, they just work.
Along the same lines, libtest has the &lt;code&gt;test::black_box&lt;/code&gt; function to prevent
unwanted constant folding, which Criterion.rs can only approximate for now.
Finally, libtest is the only option for benchmarks within your main crate -
both Criterion.rs and bencher can only be used in the &lt;code&gt;benches&lt;/code&gt; folder at
present.&lt;/p&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;I hope I&amp;rsquo;ve convinced you to give Criterion.rs a look. I&amp;rsquo;m excited for the
future of this project and of Rust as a whole, and I hope you are too.&lt;/p&gt;

&lt;p&gt;Although Criterion.rs now supports stable Rust, that doesn&amp;rsquo;t mean that it
itself is stable, or even feature-complete. I certainly plan to continue
polishing and expanding on what Criterion.rs already provides. If you&amp;rsquo;d like to
help with that effort, or if you&amp;rsquo;d like to make suggestions, feature requests
or bug reports, please check out &lt;a href=&#34;https://github.com/japaric/criterion.rs&#34;&gt;the repository on
GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In addition, I hope to work with the Rust team to help define and implement the
necessary changes to Cargo and rustc to use alternate test and benchmark
frameworks. This would make it as seamless to use Criterion.rs as it already is
to use libtest, and will hopefully allow the community to experiment with a
variety of ways to support testing and benchmarking.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are Benchmarks From Cloud CI Services Reliable?</title>
      <link>https://bheisler.github.io/post/benchmarking-in-the-cloud/</link>
      <pubDate>Sat, 06 Jan 2018 16:00:00 -0600</pubDate>
      
      <guid>https://bheisler.github.io/post/benchmarking-in-the-cloud/</guid>
      <description>

&lt;p&gt;After I released the first version of &lt;a href=&#34;https://github.com/japaric/criterion.rs&#34;&gt;Criterion.rs&lt;/a&gt;,
(a statistics-driven benchmarking tool for Rust) I was asked about using it to
detect performance regressions as part of a cloud-based continuous integration
(CI) pipeline such as Travis-CI or Appveyor. That got me wondering - does it
even make sense to do that?&lt;/p&gt;

&lt;p&gt;Cloud-CI pipelines have a lot of potential to introduce noise into the benchmarking
process - unpredictable load on the physical hosts of the build VM&amp;rsquo;s, or
even unpredictable migrations of VMs between physical hosts. How much
noise is there really, and how much does it affect real-world benchmarks? I
couldn&amp;rsquo;t find any attempt to answer that question with real data, so I decided
to do it myself.&lt;/p&gt;

&lt;p&gt;tl;dr: Yes, there is enough noise to make benchmark results unreliable.
Read on if you want to see the numbers.&lt;/p&gt;

&lt;p&gt;In this post, I benchmarked on Travis-CI, but I don&amp;rsquo;t mean to single them out,
they&amp;rsquo;re just the cloud-CI provider that I&amp;rsquo;m most familiar with.
To the best of my knowledge, they don&amp;rsquo;t claim that their service is suitable
for benchmarking.&lt;/p&gt;

&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;Before I can test the effects of the cloud-CI environment on benchmarks, I need
some benchmarks. I opted to use the existing benchmark suite of Rust&amp;rsquo;s
&lt;a href=&#34;https://github.com/rust-lang/regex&#34;&gt;regex library&lt;/a&gt;, because it&amp;rsquo;s a well-known,
well-regarded and high-performance codebase. Specifically, I used the &amp;ldquo;rust&amp;rdquo;
benchmark suite. The regex project&amp;rsquo;s benchmarks use Rust&amp;rsquo;s standard &lt;a href=&#34;https://github.com/rust-lang/rust/tree/master/src/libtest&#34;&gt;&lt;code&gt;libtest&lt;/code&gt;
benchmark/test harness&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I ran the benchmarks in pairs, as suggested in &lt;a href=&#34;https://beachape.com/blog/2016/11/02/rust-performance-testing-on-travis-ci/&#34;&gt;this post by BeachApe&lt;/a&gt;.
However, that post suggests running one benchmark with master and one with a
pull-request branch - all of my benchmarks were done with the same version of the
code to prevent changes to the code from affecting the results. For the cloud
benchmarks, each pair was run in a separate build job on Travis-CI.&lt;/p&gt;

&lt;p&gt;I wrote a script to run 100 such pairs of builds on an old desktop machine I
had laying around, and another to run 100 Travis-CI builds by editing, committing
and force-pushing an unused file, then downloading the resulting build log.
Note that I did edit the Travis build script to only perform the necessary
compilation and benchmarking, to avoid using more of Travis-CI&amp;rsquo;s resources than
was necessary. A few of the resulting log files were damaged and were replaced
with log files from new builds at the end. There were a number of occasions where
parts of the logs from Travis-CI were missing or corrupted and I am not certain
that I found all of them.&lt;/p&gt;

&lt;p&gt;Each pair was then compared using &lt;a href=&#34;https://github.com/BurntSushi/cargo-benchcmp&#34;&gt;cargo benchcmp&lt;/a&gt;
and the percentage differences were extracted with more scripts.&lt;/p&gt;

&lt;p&gt;The pairwise benchmarking approach has a few advantages. First, by running both
benchmarks on the same physical machine (for local benchmarks) or the same
build job (for cloud benchmarks), all effects which are constant for the length
of a benchmark pair can be ignored. This includes differences in the performance
of the physical hosts or differences in compiler versions, since we&amp;rsquo;re only
looking at the percentage change between two consecutive benchmarks. Using the
percentage differences also controls for some benchmarks naturally taking longer
than others.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://bheisler.github.io/static/travis_benchmark_histogram.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;As you can see from the above chart, the cloud benchmarks do indeed show more
noise than the local benchmarks.&lt;/p&gt;

&lt;p&gt;All numbers are in units of percentage points representing the percentage
difference between the two benchmarks of a pair:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Local:
mean: 0.0315897959184
min: -24.6, max: 22.18
std. dev.: 2.11411179379

Cloud:
mean: 1.42961542492
min: -99.99, max: 3177.03
std. dev.: 72.1539676978

Levene&#39;s Test p-value: 1.97E-49
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that there were four benchmark results in the cloud set with percentage
differences greater than 10,000% which I&amp;rsquo;ve removed as outliers. Those were not
included in the calculations above; if they were included the cloud numbers would be
substantially worse. I opted to remove them after inspecting them and finding
inconsistencies in those benchmark results which lead me to suspect that the
logs were damaged. For example, one benchmark shows the time for each iteration
increased by more than 200x but the throughput for the same benchmark
appears to have increased slightly, rather than decreased as one would expect.&lt;/p&gt;

&lt;p&gt;Additionally, manual inspection of the comparison results shows that sometimes
multiple consecutive benchmark tests within a single run of the benchmarks
all differ from their pair by a large and consistent value. This could indicate
something is slowing down the build VM by a significant degree and persisting
long enough to affect multiple benchmark tests.&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;The greatly increased variance of benchmarks done in the cloud casts doubt on
the reliability of benchmarks performed on cloud-CI pipelines. This confirms the
intuitive expectation.&lt;/p&gt;

&lt;p&gt;To be clear; this doesn&amp;rsquo;t mean every benchmark is wrong - many of the comparisons
show shifts of +-2%, roughly similar to the noise observed in local benchmarks.
However, differences of as much as 50% are fairly common with no change
in the code at all, which makes it very difficult to know if a change
in benchmarking results is due to a change in the true performance of the code
being benchmarked, or if it is simply noise. Hence, unreliable.&lt;/p&gt;

&lt;p&gt;It would still be useful to have automated detection of performance regressions
as part of a CI pipeline, however. Further work is needed to find ways to
mitigate the effects of this noise.&lt;/p&gt;

&lt;p&gt;One way to reduce noise in this system would be to execute each benchmark suite
two or more times with each version of the code and accept the one with the
smallest mean or variance before comparing the two. In this case, it would be
best to run each benchmark suite to completion before running it again rather
than running each test twice consecutively, to reduce the chance that some
external influence affects a single test twice.&lt;/p&gt;

&lt;p&gt;A simpler, though more manual, method to accomplish the same thing would be to
run the whole benchmarking process in multiple build jobs. In that case, before
merging a pull request, a maintainer could manually examine the results. If a
performance regression is detected by all of the build jobs, it&amp;rsquo;s probably safe
to treat it as real rather than noise.&lt;/p&gt;

&lt;p&gt;It is also possible that different cloud-CI providers could make for less noisy
benchmarking environments, though I haven&amp;rsquo;t measured that.&lt;/p&gt;

&lt;p&gt;All of the data and analysis scripts can be found &lt;a href=&#34;https://github.com/bheisler/travis-benchmark-data&#34;&gt;on GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you to Daniel Hogan, for reading over this post and giving me a great deal
of useful feedback. I&amp;rsquo;d also like to thank Andrew Gallant (@burntsushi) and co.
for creating both the regex crate and cargo-benchcmp.&lt;/p&gt;

&lt;h2 id=&#34;addendum-why-libtest-and-not-criterion-rs&#34;&gt;Addendum: Why libtest and not Criterion.rs&lt;/h2&gt;

&lt;p&gt;I opted to use Rust&amp;rsquo;s standard benchmarking tool rather than Criterion.rs because
there are no large, well-regarded projects using Criterion.rs to perform their
benchmarks at present.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t know whether using Criterion.rs would change these results or not.
Criterion&amp;rsquo;s analysis process is different enough that it might, but until I have
data one way or another I intend to advise users not to trust cloud benchmarks
based on Criterion.rs.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>